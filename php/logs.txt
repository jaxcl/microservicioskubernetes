
==> Audit <==
|------------|-----------------------|----------|---------------------|---------|----------------------|----------------------|
|  Command   |         Args          | Profile  |        User         | Version |      Start Time      |       End Time       |
|------------|-----------------------|----------|---------------------|---------|----------------------|----------------------|
| start      |                       | minikube | T23W03\CursosTardes | v1.33.1 | 12 Jun 24 17:14 CEST | 12 Jun 24 17:15 CEST |
| start      |                       | minikube | T23W03\CursosTardes | v1.33.1 | 12 Jun 24 18:05 CEST | 12 Jun 24 18:06 CEST |
| docker-env |                       | minikube | T23W03\CursosTardes | v1.33.1 | 12 Jun 24 18:06 CEST | 12 Jun 24 18:06 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 16:53 CEST |                      |
| start      |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 16:54 CEST | 14 Jun 24 16:55 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 16:55 CEST | 14 Jun 24 16:55 CEST |
| docker-env |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:10 CEST | 14 Jun 24 17:10 CEST |
| docker-env |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:19 CEST | 14 Jun 24 17:19 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:19 CEST | 14 Jun 24 17:19 CEST |
| addons     | enable metrics-server | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:20 CEST | 14 Jun 24 17:20 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:21 CEST | 14 Jun 24 17:21 CEST |
| docker-env |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:51 CEST | 14 Jun 24 17:51 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:51 CEST | 14 Jun 24 17:51 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:52 CEST | 14 Jun 24 17:52 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 17:56 CEST | 14 Jun 24 17:56 CEST |
| docker-env |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 18:44 CEST | 14 Jun 24 18:44 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 18:45 CEST | 14 Jun 24 18:45 CEST |
| addons     | enable metrics-server | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 18:45 CEST | 14 Jun 24 18:45 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 18:47 CEST | 14 Jun 24 18:47 CEST |
| ip         |                       | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 18:48 CEST | 14 Jun 24 18:48 CEST |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:06 CEST | 14 Jun 24 19:18 CEST |
| service    | webapp                | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:07 CEST |                      |
| service    | webapp                | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:11 CEST |                      |
| service    | webapp                | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:14 CEST |                      |
| service    | my-webapp             | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:18 CEST | 14 Jun 24 19:24 CEST |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:18 CEST | 14 Jun 24 19:26 CEST |
| service    | my-webapp             | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:24 CEST |                      |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:27 CEST | 14 Jun 24 19:29 CEST |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:29 CEST |                      |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:30 CEST |                      |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:31 CEST | 14 Jun 24 19:33 CEST |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:35 CEST |                      |
| service    | php-service           | minikube | T23W03\CursosTardes | v1.33.1 | 14 Jun 24 19:37 CEST |                      |
|------------|-----------------------|----------|---------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/06/14 16:54:40
Running on machine: T23W03
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0614 16:54:40.405658   14456 out.go:291] Setting OutFile to fd 300 ...
I0614 16:54:40.405986   14456 out.go:338] TERM=xterm,COLORTERM=, which probably does not support color
I0614 16:54:40.405986   14456 out.go:304] Setting ErrFile to fd 300...
I0614 16:54:40.405986   14456 out.go:338] TERM=xterm,COLORTERM=, which probably does not support color
W0614 16:54:40.415847   14456 root.go:314] Error reading config file at C:\Users\CursosTardes\.minikube\config\config.json: open C:\Users\CursosTardes\.minikube\config\config.json: El sistema no puede encontrar el archivo especificado.
I0614 16:54:40.421464   14456 out.go:298] Setting JSON to false
I0614 16:54:40.423863   14456 start.go:129] hostinfo: {"hostname":"T23W03","uptime":2115,"bootTime":1718374765,"procs":251,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22000.2538 Build 22000.2538","kernelVersion":"10.0.22000.2538 Build 22000.2538","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"096dd451-5c36-421f-91e8-5fff48408dd9"}
W0614 16:54:40.423863   14456 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0614 16:54:40.425752   14456 out.go:177] * minikube v1.33.1 en Microsoft Windows 11 Pro 10.0.22000.2538 Build 22000.2538
I0614 16:54:40.426416   14456 notify.go:220] Checking for updates...
I0614 16:54:40.427035   14456 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0614 16:54:40.428076   14456 driver.go:392] Setting default libvirt URI to qemu:///system
I0614 16:54:40.573266   14456 docker.go:122] docker version: linux-24.0.7:Docker Desktop 4.26.0 (130397)
I0614 16:54:40.576375   14456 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0614 16:54:42.225687   14456 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.6493115s)
I0614 16:54:42.229066   14456 info.go:266] docker info: {ID:2dced299-f587-48d0-ae38-8b36f935206b Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:true NGoroutines:57 SystemTime:2024-06-14 14:54:42.195076145 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269144064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0614 16:54:42.234030   14456 out.go:177] * Using the docker driver based on existing profile
I0614 16:54:42.236303   14456 start.go:297] selected driver: docker
I0614 16:54:42.236303   14456 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CursosTardes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0614 16:54:42.236808   14456 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0614 16:54:42.241885   14456 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0614 16:54:42.490264   14456 info.go:266] docker info: {ID:2dced299-f587-48d0-ae38-8b36f935206b Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:true NGoroutines:57 SystemTime:2024-06-14 14:54:42.460038256 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269144064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0614 16:54:42.518608   14456 cni.go:84] Creating CNI manager for ""
I0614 16:54:42.519901   14456 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0614 16:54:42.520407   14456 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CursosTardes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0614 16:54:42.525632   14456 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0614 16:54:42.528274   14456 cache.go:121] Beginning downloading kic base image for docker with docker
I0614 16:54:42.529819   14456 out.go:177] * Pulling base image v0.0.44 ...
I0614 16:54:42.531929   14456 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0614 16:54:42.531929   14456 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0614 16:54:42.532458   14456 preload.go:147] Found local preload: C:\Users\CursosTardes\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0614 16:54:42.532458   14456 cache.go:56] Caching tarball of preloaded images
I0614 16:54:42.532998   14456 preload.go:173] Found C:\Users\CursosTardes\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0614 16:54:42.532998   14456 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0614 16:54:42.533547   14456 profile.go:143] Saving config to C:\Users\CursosTardes\.minikube\profiles\minikube\config.json ...
I0614 16:54:42.644468   14456 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0614 16:54:42.644468   14456 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0614 16:54:42.646059   14456 cache.go:194] Successfully downloaded all kic artifacts
I0614 16:54:42.646580   14456 start.go:360] acquireMachinesLock for minikube: {Name:mkf53f641f970a7b2804bc20a1ab9ff72cfb7f0f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0614 16:54:42.647157   14456 start.go:364] duration metric: took 576.9µs to acquireMachinesLock for "minikube"
I0614 16:54:42.647157   14456 start.go:96] Skipping create...Using existing machine configuration
I0614 16:54:42.647681   14456 fix.go:54] fixHost starting: 
I0614 16:54:42.652965   14456 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0614 16:54:42.752104   14456 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0614 16:54:42.754722   14456 fix.go:138] unexpected machine state, will restart: <nil>
I0614 16:54:42.755295   14456 out.go:177] * Restarting existing docker container for "minikube" ...
I0614 16:54:42.759546   14456 cli_runner.go:164] Run: docker start minikube
I0614 16:54:43.289482   14456 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0614 16:54:43.441375   14456 kic.go:430] container "minikube" state is running.
I0614 16:54:43.449448   14456 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0614 16:54:43.593036   14456 profile.go:143] Saving config to C:\Users\CursosTardes\.minikube\profiles\minikube\config.json ...
I0614 16:54:43.594609   14456 machine.go:94] provisionDockerMachine start ...
I0614 16:54:43.598865   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:43.721519   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:43.736530   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:43.736530   14456 main.go:141] libmachine: About to run SSH command:
hostname
I0614 16:54:43.739782   14456 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0614 16:54:46.918084   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0614 16:54:46.918084   14456 ubuntu.go:169] provisioning hostname "minikube"
I0614 16:54:46.922067   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:47.040089   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:47.040089   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:47.040089   14456 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0614 16:54:47.252711   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0614 16:54:47.256629   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:47.366835   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:47.366835   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:47.366835   14456 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0614 16:54:47.547789   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0614 16:54:47.547850   14456 ubuntu.go:175] set auth options {CertDir:C:\Users\CursosTardes\.minikube CaCertPath:C:\Users\CursosTardes\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\CursosTardes\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\CursosTardes\.minikube\machines\server.pem ServerKeyPath:C:\Users\CursosTardes\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\CursosTardes\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\CursosTardes\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\CursosTardes\.minikube}
I0614 16:54:47.547850   14456 ubuntu.go:177] setting up certificates
I0614 16:54:47.547850   14456 provision.go:84] configureAuth start
I0614 16:54:47.553617   14456 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0614 16:54:47.660714   14456 provision.go:143] copyHostCerts
I0614 16:54:47.672816   14456 exec_runner.go:144] found C:\Users\CursosTardes\.minikube/cert.pem, removing ...
I0614 16:54:47.672816   14456 exec_runner.go:203] rm: C:\Users\CursosTardes\.minikube\cert.pem
I0614 16:54:47.673322   14456 exec_runner.go:151] cp: C:\Users\CursosTardes\.minikube\certs\cert.pem --> C:\Users\CursosTardes\.minikube/cert.pem (1139 bytes)
I0614 16:54:47.681044   14456 exec_runner.go:144] found C:\Users\CursosTardes\.minikube/key.pem, removing ...
I0614 16:54:47.681044   14456 exec_runner.go:203] rm: C:\Users\CursosTardes\.minikube\key.pem
I0614 16:54:47.681591   14456 exec_runner.go:151] cp: C:\Users\CursosTardes\.minikube\certs\key.pem --> C:\Users\CursosTardes\.minikube/key.pem (1679 bytes)
I0614 16:54:47.687954   14456 exec_runner.go:144] found C:\Users\CursosTardes\.minikube/ca.pem, removing ...
I0614 16:54:47.687954   14456 exec_runner.go:203] rm: C:\Users\CursosTardes\.minikube\ca.pem
I0614 16:54:47.687954   14456 exec_runner.go:151] cp: C:\Users\CursosTardes\.minikube\certs\ca.pem --> C:\Users\CursosTardes\.minikube/ca.pem (1094 bytes)
I0614 16:54:47.687954   14456 provision.go:117] generating server cert: C:\Users\CursosTardes\.minikube\machines\server.pem ca-key=C:\Users\CursosTardes\.minikube\certs\ca.pem private-key=C:\Users\CursosTardes\.minikube\certs\ca-key.pem org=CursosTardes.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0614 16:54:47.808285   14456 provision.go:177] copyRemoteCerts
I0614 16:54:47.813793   14456 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0614 16:54:47.813793   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:47.926088   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:48.042814   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0614 16:54:48.077253   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0614 16:54:48.111444   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0614 16:54:48.144080   14456 provision.go:87] duration metric: took 595.6851ms to configureAuth
I0614 16:54:48.144080   14456 ubuntu.go:193] setting minikube options for container-runtime
I0614 16:54:48.144679   14456 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0614 16:54:48.148565   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:48.283329   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:48.283329   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:48.283329   14456 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0614 16:54:48.447616   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0614 16:54:48.447616   14456 ubuntu.go:71] root file system type: overlay
I0614 16:54:48.448727   14456 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0614 16:54:48.452031   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:48.578022   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:48.578613   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:48.578613   14456 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0614 16:54:48.771660   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0614 16:54:48.776609   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:48.886754   14456 main.go:141] libmachine: Using SSH client type: native
I0614 16:54:48.886754   14456 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x53a3c0] 0x53cfa0 <nil>  [] 0s} 127.0.0.1 52051 <nil> <nil>}
I0614 16:54:48.886754   14456 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0614 16:54:49.051975   14456 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0614 16:54:49.051975   14456 machine.go:97] duration metric: took 5.4573663s to provisionDockerMachine
I0614 16:54:49.051975   14456 start.go:293] postStartSetup for "minikube" (driver="docker")
I0614 16:54:49.051975   14456 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0614 16:54:49.057216   14456 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0614 16:54:49.061523   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:49.178215   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:49.293925   14456 ssh_runner.go:195] Run: cat /etc/os-release
I0614 16:54:49.301015   14456 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0614 16:54:49.301015   14456 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0614 16:54:49.301015   14456 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0614 16:54:49.301015   14456 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0614 16:54:49.301015   14456 filesync.go:126] Scanning C:\Users\CursosTardes\.minikube\addons for local assets ...
I0614 16:54:49.301556   14456 filesync.go:126] Scanning C:\Users\CursosTardes\.minikube\files for local assets ...
I0614 16:54:49.301556   14456 start.go:296] duration metric: took 249.5808ms for postStartSetup
I0614 16:54:49.302693   14456 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0614 16:54:49.305426   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:49.427476   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:49.547342   14456 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0614 16:54:49.555830   14456 fix.go:56] duration metric: took 6.9086736s for fixHost
I0614 16:54:49.555830   14456 start.go:83] releasing machines lock for "minikube", held for 6.9086736s
I0614 16:54:49.559178   14456 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0614 16:54:49.675045   14456 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0614 16:54:49.675657   14456 ssh_runner.go:195] Run: cat /version.json
I0614 16:54:49.679506   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:49.680587   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:49.813776   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:49.829310   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:49.921294   14456 ssh_runner.go:195] Run: systemctl --version
I0614 16:54:50.193868   14456 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0614 16:54:50.206214   14456 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0614 16:54:50.222101   14456 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0614 16:54:50.226447   14456 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0614 16:54:50.240364   14456 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0614 16:54:50.240364   14456 start.go:494] detecting cgroup driver to use...
I0614 16:54:50.240364   14456 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0614 16:54:50.243076   14456 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0614 16:54:50.265279   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0614 16:54:50.280673   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0614 16:54:50.295766   14456 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0614 16:54:50.296899   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0614 16:54:50.314284   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0614 16:54:50.330418   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0614 16:54:50.345109   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0614 16:54:50.361114   14456 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0614 16:54:50.376095   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0614 16:54:50.395221   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0614 16:54:50.411112   14456 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0614 16:54:50.431767   14456 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0614 16:54:50.449891   14456 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0614 16:54:50.467583   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:50.607575   14456 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0614 16:54:50.751318   14456 start.go:494] detecting cgroup driver to use...
I0614 16:54:50.751318   14456 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0614 16:54:50.758842   14456 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0614 16:54:50.776576   14456 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0614 16:54:50.782471   14456 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0614 16:54:50.797488   14456 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0614 16:54:50.828602   14456 ssh_runner.go:195] Run: which cri-dockerd
I0614 16:54:50.846218   14456 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0614 16:54:50.860719   14456 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0614 16:54:50.899633   14456 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0614 16:54:51.200790   14456 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0614 16:54:51.473622   14456 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0614 16:54:51.474163   14456 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0614 16:54:51.503210   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:51.617129   14456 ssh_runner.go:195] Run: sudo systemctl restart docker
I0614 16:54:52.012418   14456 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0614 16:54:52.031021   14456 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0614 16:54:52.052707   14456 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0614 16:54:52.072704   14456 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0614 16:54:52.185484   14456 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0614 16:54:52.314138   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:52.426493   14456 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0614 16:54:52.449472   14456 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0614 16:54:52.468203   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:52.607942   14456 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0614 16:54:53.117285   14456 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0614 16:54:53.118349   14456 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0614 16:54:53.124861   14456 start.go:562] Will wait 60s for crictl version
I0614 16:54:53.126116   14456 ssh_runner.go:195] Run: which crictl
I0614 16:54:53.135967   14456 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0614 16:54:53.410556   14456 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0614 16:54:53.414465   14456 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0614 16:54:53.634718   14456 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0614 16:54:53.669351   14456 out.go:204] * Preparando Kubernetes v1.30.0 en Docker 26.1.1...
I0614 16:54:53.673191   14456 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0614 16:54:53.900560   14456 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0614 16:54:53.901075   14456 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0614 16:54:53.907629   14456 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0614 16:54:53.925475   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0614 16:54:54.041398   14456 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CursosTardes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0614 16:54:54.041937   14456 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0614 16:54:54.044668   14456 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0614 16:54:54.070349   14456 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0614 16:54:54.070349   14456 docker.go:615] Images already preloaded, skipping extraction
I0614 16:54:54.074112   14456 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0614 16:54:54.102835   14456 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0614 16:54:54.102835   14456 cache_images.go:84] Images are preloaded, skipping loading
I0614 16:54:54.102835   14456 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0614 16:54:54.103972   14456 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0614 16:54:54.107740   14456 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0614 16:54:54.492320   14456 cni.go:84] Creating CNI manager for ""
I0614 16:54:54.492320   14456 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0614 16:54:54.493188   14456 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0614 16:54:54.493188   14456 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0614 16:54:54.494630   14456 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0614 16:54:54.498365   14456 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0614 16:54:54.514446   14456 binaries.go:44] Found k8s binaries, skipping transfer
I0614 16:54:54.518685   14456 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0614 16:54:54.531267   14456 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0614 16:54:54.555934   14456 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0614 16:54:54.577899   14456 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0614 16:54:54.603553   14456 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0614 16:54:54.610296   14456 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0614 16:54:54.631310   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:54.767199   14456 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0614 16:54:54.789018   14456 certs.go:68] Setting up C:\Users\CursosTardes\.minikube\profiles\minikube for IP: 192.168.49.2
I0614 16:54:54.789018   14456 certs.go:194] generating shared ca certs ...
I0614 16:54:54.789018   14456 certs.go:226] acquiring lock for ca certs: {Name:mkee976aba7ddf176ea6e738ec85c61226c3862b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0614 16:54:54.800302   14456 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\CursosTardes\.minikube\ca.key
I0614 16:54:54.814631   14456 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\CursosTardes\.minikube\proxy-client-ca.key
I0614 16:54:54.815498   14456 certs.go:256] generating profile certs ...
I0614 16:54:54.816201   14456 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\CursosTardes\.minikube\profiles\minikube\client.key
I0614 16:54:54.833779   14456 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\CursosTardes\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0614 16:54:54.846955   14456 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\CursosTardes\.minikube\profiles\minikube\proxy-client.key
I0614 16:54:54.850984   14456 certs.go:484] found cert: C:\Users\CursosTardes\.minikube\certs\ca-key.pem (1679 bytes)
I0614 16:54:54.850984   14456 certs.go:484] found cert: C:\Users\CursosTardes\.minikube\certs\ca.pem (1094 bytes)
I0614 16:54:54.850984   14456 certs.go:484] found cert: C:\Users\CursosTardes\.minikube\certs\cert.pem (1139 bytes)
I0614 16:54:54.851506   14456 certs.go:484] found cert: C:\Users\CursosTardes\.minikube\certs\key.pem (1679 bytes)
I0614 16:54:54.869478   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0614 16:54:54.903379   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0614 16:54:54.936998   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0614 16:54:54.970400   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0614 16:54:55.006822   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0614 16:54:55.042180   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0614 16:54:55.074837   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0614 16:54:55.112473   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0614 16:54:55.152510   14456 ssh_runner.go:362] scp C:\Users\CursosTardes\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0614 16:54:55.193851   14456 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0614 16:54:55.224911   14456 ssh_runner.go:195] Run: openssl version
I0614 16:54:55.254522   14456 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0614 16:54:55.273918   14456 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0614 16:54:55.280933   14456 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 12 15:14 /usr/share/ca-certificates/minikubeCA.pem
I0614 16:54:55.281482   14456 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0614 16:54:55.294315   14456 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0614 16:54:55.308201   14456 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0614 16:54:55.315810   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0614 16:54:55.326450   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0614 16:54:55.340623   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0614 16:54:55.351920   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0614 16:54:55.362027   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0614 16:54:55.371827   14456 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0614 16:54:55.382807   14456 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CursosTardes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0614 16:54:55.385922   14456 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0614 16:54:55.419195   14456 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0614 16:54:55.430808   14456 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0614 16:54:55.430808   14456 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0614 16:54:55.432378   14456 kubeadm.go:587] restartPrimaryControlPlane start ...
I0614 16:54:55.435997   14456 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0614 16:54:55.449003   14456 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0614 16:54:55.452230   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0614 16:54:55.583479   14456 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:51626"
I0614 16:54:55.583479   14456 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:51626, want: 127.0.0.1:52050
I0614 16:54:55.584004   14456 kubeconfig.go:62] C:\Users\CursosTardes\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0614 16:54:55.584542   14456 lock.go:35] WriteFile acquiring C:\Users\CursosTardes\.kube\config: {Name:mkf07551a9e6f3d30ae569a10dfe377a33b6e5b3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0614 16:54:55.616775   14456 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0614 16:54:55.639747   14456 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0614 16:54:55.639747   14456 kubeadm.go:591] duration metric: took 207.369ms to restartPrimaryControlPlane
I0614 16:54:55.639747   14456 kubeadm.go:393] duration metric: took 256.9393ms to StartCluster
I0614 16:54:55.639747   14456 settings.go:142] acquiring lock: {Name:mka5f98bedbf44147a972505e814f943a4234c5c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0614 16:54:55.639747   14456 settings.go:150] Updating kubeconfig:  C:\Users\CursosTardes\.kube\config
I0614 16:54:55.641427   14456 lock.go:35] WriteFile acquiring C:\Users\CursosTardes\.kube\config: {Name:mkf07551a9e6f3d30ae569a10dfe377a33b6e5b3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0614 16:54:55.644708   14456 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0614 16:54:55.644708   14456 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0614 16:54:55.645899   14456 out.go:177] * Verifying Kubernetes components...
I0614 16:54:55.644708   14456 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0614 16:54:55.645899   14456 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0614 16:54:55.645899   14456 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0614 16:54:55.646439   14456 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0614 16:54:55.646439   14456 addons.go:243] addon storage-provisioner should already be in state true
I0614 16:54:55.648090   14456 host.go:66] Checking if "minikube" exists ...
I0614 16:54:55.646985   14456 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0614 16:54:55.656094   14456 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0614 16:54:55.659391   14456 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0614 16:54:55.661556   14456 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0614 16:54:55.795879   14456 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0614 16:54:55.796923   14456 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0614 16:54:55.796923   14456 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0614 16:54:55.802308   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:55.812895   14456 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0614 16:54:55.812895   14456 addons.go:243] addon default-storageclass should already be in state true
I0614 16:54:55.812895   14456 host.go:66] Checking if "minikube" exists ...
I0614 16:54:55.818661   14456 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0614 16:54:55.907228   14456 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0614 16:54:55.932540   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0614 16:54:55.934131   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:55.950724   14456 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0614 16:54:55.950724   14456 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0614 16:54:55.960973   14456 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0614 16:54:56.073710   14456 api_server.go:52] waiting for apiserver process to appear ...
I0614 16:54:56.085159   14456 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0614 16:54:56.094135   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0614 16:54:56.104469   14456 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52051 SSHKeyPath:C:\Users\CursosTardes\.minikube\machines\minikube\id_rsa Username:docker}
I0614 16:54:56.246533   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0614 16:54:56.587306   14456 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0614 16:54:56.992256   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0614 16:54:56.992256   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:56.994506   14456 retry.go:31] will retry after 289.222551ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:56.994506   14456 retry.go:31] will retry after 349.90585ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:57.095105   14456 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0614 16:54:57.296032   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0614 16:54:57.359997   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0614 16:54:57.405104   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:57.405104   14456 retry.go:31] will retry after 376.393616ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0614 16:54:57.448247   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:57.448247   14456 retry.go:31] will retry after 423.924254ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:57.594491   14456 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0614 16:54:57.614131   14456 api_server.go:72] duration metric: took 1.9694223s to wait for apiserver process to appear ...
I0614 16:54:57.619467   14456 api_server.go:88] waiting for apiserver healthz status ...
I0614 16:54:57.619467   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:54:57.631199   14456 api_server.go:269] stopped: https://127.0.0.1:52050/healthz: Get "https://127.0.0.1:52050/healthz": EOF
I0614 16:54:57.800374   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0614 16:54:57.895101   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0614 16:54:57.902220   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:57.902220   14456 retry.go:31] will retry after 545.992418ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0614 16:54:58.059393   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:58.059393   14456 retry.go:31] will retry after 773.978216ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:58.122019   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:54:58.123850   14456 api_server.go:269] stopped: https://127.0.0.1:52050/healthz: Get "https://127.0.0.1:52050/healthz": EOF
I0614 16:54:58.455085   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0614 16:54:58.541141   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:58.541141   14456 retry.go:31] will retry after 877.173532ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:58.623016   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:54:58.625151   14456 api_server.go:269] stopped: https://127.0.0.1:52050/healthz: Get "https://127.0.0.1:52050/healthz": EOF
I0614 16:54:58.845976   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0614 16:54:58.927858   14456 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:58.927858   14456 retry.go:31] will retry after 551.082525ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0614 16:54:59.125156   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:54:59.126775   14456 api_server.go:269] stopped: https://127.0.0.1:52050/healthz: Get "https://127.0.0.1:52050/healthz": EOF
I0614 16:54:59.428832   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0614 16:54:59.490829   14456 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0614 16:54:59.627682   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:01.993063   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0614 16:55:01.993063   14456 api_server.go:103] status: https://127.0.0.1:52050/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0614 16:55:01.993063   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:02.048462   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0614 16:55:02.048462   14456 api_server.go:103] status: https://127.0.0.1:52050/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0614 16:55:02.121241   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:02.154426   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0614 16:55:02.154426   14456 api_server.go:103] status: https://127.0.0.1:52050/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0614 16:55:02.623328   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:02.642396   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0614 16:55:02.642396   14456 api_server.go:103] status: https://127.0.0.1:52050/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0614 16:55:03.128095   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:03.142779   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0614 16:55:03.142779   14456 api_server.go:103] status: https://127.0.0.1:52050/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0614 16:55:03.344249   14456 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.915417s)
I0614 16:55:03.344249   14456 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.8534202s)
I0614 16:55:03.377107   14456 out.go:177] * Complementos habilitados: storage-provisioner, default-storageclass
I0614 16:55:03.378440   14456 addons.go:505] duration metric: took 7.7342707s for enable addons: enabled=[storage-provisioner default-storageclass]
I0614 16:55:03.629055   14456 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52050/healthz ...
I0614 16:55:03.636728   14456 api_server.go:279] https://127.0.0.1:52050/healthz returned 200:
ok
I0614 16:55:03.638333   14456 api_server.go:141] control plane version: v1.30.0
I0614 16:55:03.638864   14456 api_server.go:131] duration metric: took 6.0188665s to wait for apiserver health ...
I0614 16:55:03.638864   14456 system_pods.go:43] waiting for kube-system pods to appear ...
I0614 16:55:03.656652   14456 system_pods.go:59] 7 kube-system pods found
I0614 16:55:03.656652   14456 system_pods.go:61] "coredns-7db6d8ff4d-6cf9t" [3b6df209-7d88-41dd-8a79-723e3ed652c7] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "etcd-minikube" [f3c2efcf-659a-46f9-8255-04ad9fb55067] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "kube-apiserver-minikube" [98932bfe-92b6-49b6-a368-917cd23d6aed] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "kube-controller-manager-minikube" [b3f7e3c2-a1a9-4d8c-97dd-0b146daba3bf] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "kube-proxy-j75p7" [99603fce-3e88-4eb1-b920-57808bb3a32b] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "kube-scheduler-minikube" [9bccc441-1411-4533-a8bb-8393f9951bbc] Running
I0614 16:55:03.656652   14456 system_pods.go:61] "storage-provisioner" [f45f7a7a-dcd2-4f3a-87cb-f7b3f7c08aaa] Running
I0614 16:55:03.656652   14456 system_pods.go:74] duration metric: took 17.7879ms to wait for pod list to return data ...
I0614 16:55:03.656652   14456 kubeadm.go:576] duration metric: took 8.011944s to wait for: map[apiserver:true system_pods:true]
I0614 16:55:03.656652   14456 node_conditions.go:102] verifying NodePressure condition ...
I0614 16:55:03.663513   14456 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0614 16:55:03.663513   14456 node_conditions.go:123] node cpu capacity is 8
I0614 16:55:03.663513   14456 node_conditions.go:105] duration metric: took 6.8606ms to run NodePressure ...
I0614 16:55:03.663513   14456 start.go:240] waiting for startup goroutines ...
I0614 16:55:03.663513   14456 start.go:245] waiting for cluster config update ...
I0614 16:55:03.663513   14456 start.go:254] writing updated cluster config ...
I0614 16:55:03.665126   14456 ssh_runner.go:195] Run: rm -f paused
I0614 16:55:03.763081   14456 start.go:600] kubectl: 1.29.2, cluster: 1.30.0 (minor skew: 1)
I0614 16:55:03.764116   14456 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 14 15:53:58 minikube dockerd[1014]: time="2024-06-14T15:53:58.503867190Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=6a0631a8c280a09b traceID=c8ada13252d17b896dbd90a98271aa13
Jun 14 15:53:58 minikube dockerd[1014]: time="2024-06-14T15:53:58.509007007Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=6a0631a8c280a09b traceID=c8ada13252d17b896dbd90a98271aa13
Jun 14 15:56:49 minikube dockerd[1014]: time="2024-06-14T15:56:49.490738489Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=fa4143c1c890be7e traceID=e441943081c91e1621f35f51d39b282d
Jun 14 15:56:49 minikube dockerd[1014]: time="2024-06-14T15:56:49.490938892Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=fa4143c1c890be7e traceID=e441943081c91e1621f35f51d39b282d
Jun 14 15:56:49 minikube dockerd[1014]: time="2024-06-14T15:56:49.496268562Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=fa4143c1c890be7e traceID=e441943081c91e1621f35f51d39b282d
Jun 14 16:01:56 minikube dockerd[1014]: time="2024-06-14T16:01:56.475706638Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=2d66099710c413ff traceID=9249fe96d18d66e2791bb95e4d89a594
Jun 14 16:01:56 minikube dockerd[1014]: time="2024-06-14T16:01:56.475774440Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=2d66099710c413ff traceID=9249fe96d18d66e2791bb95e4d89a594
Jun 14 16:01:56 minikube dockerd[1014]: time="2024-06-14T16:01:56.479053355Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=2d66099710c413ff traceID=9249fe96d18d66e2791bb95e4d89a594
Jun 14 16:07:06 minikube dockerd[1014]: time="2024-06-14T16:07:06.473447201Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=6ae6b76014a4ef93 traceID=327157e82b674732db0aae1fb7771277
Jun 14 16:07:06 minikube dockerd[1014]: time="2024-06-14T16:07:06.473584604Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=6ae6b76014a4ef93 traceID=327157e82b674732db0aae1fb7771277
Jun 14 16:07:06 minikube dockerd[1014]: time="2024-06-14T16:07:06.476970076Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=6ae6b76014a4ef93 traceID=327157e82b674732db0aae1fb7771277
Jun 14 16:12:11 minikube dockerd[1014]: time="2024-06-14T16:12:11.480343235Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=d9b9c8b5f003770f traceID=be2ebba38669b5feb80886e2a5bc735c
Jun 14 16:12:11 minikube dockerd[1014]: time="2024-06-14T16:12:11.480505335Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=d9b9c8b5f003770f traceID=be2ebba38669b5feb80886e2a5bc735c
Jun 14 16:12:11 minikube dockerd[1014]: time="2024-06-14T16:12:11.485554032Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=d9b9c8b5f003770f traceID=be2ebba38669b5feb80886e2a5bc735c
Jun 14 16:17:12 minikube dockerd[1014]: time="2024-06-14T16:17:12.470390572Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=362b59cbafae12c1 traceID=10d612c6679f717e8afe1500b60862af
Jun 14 16:17:12 minikube dockerd[1014]: time="2024-06-14T16:17:12.470528572Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=362b59cbafae12c1 traceID=10d612c6679f717e8afe1500b60862af
Jun 14 16:17:12 minikube dockerd[1014]: time="2024-06-14T16:17:12.475495269Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=362b59cbafae12c1 traceID=10d612c6679f717e8afe1500b60862af
Jun 14 16:22:14 minikube dockerd[1014]: time="2024-06-14T16:22:14.465266945Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=07acec3212d3566a traceID=6ec9473eb3049d46815da4b6bb1377fc
Jun 14 16:22:14 minikube dockerd[1014]: time="2024-06-14T16:22:14.465321744Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=07acec3212d3566a traceID=6ec9473eb3049d46815da4b6bb1377fc
Jun 14 16:22:14 minikube dockerd[1014]: time="2024-06-14T16:22:14.467737723Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=07acec3212d3566a traceID=6ec9473eb3049d46815da4b6bb1377fc
Jun 14 16:24:34 minikube cri-dockerd[1263]: time="2024-06-14T16:24:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a8ba2f5f5fbfc02c87f2ee3ec5d8a6ab62d537804f7f2694417f9c576ace88ea/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 16:24:46 minikube cri-dockerd[1263]: time="2024-06-14T16:24:46Z" level=info msg="Pulling image phpmyadmin/phpmyadmin:latest: 0705c9c2f22d: Extracting [=========>                                         ]   19.5MB/104.3MB"
Jun 14 16:24:56 minikube cri-dockerd[1263]: time="2024-06-14T16:24:56Z" level=info msg="Pulling image phpmyadmin/phpmyadmin:latest: c70df516383c: Extracting [===========================================>       ]  9.961MB/11.43MB"
Jun 14 16:24:59 minikube cri-dockerd[1263]: time="2024-06-14T16:24:59Z" level=info msg="Stop pulling image phpmyadmin/phpmyadmin:latest: Status: Downloaded newer image for phpmyadmin/phpmyadmin:latest"
Jun 14 16:27:18 minikube dockerd[1014]: time="2024-06-14T16:27:18.457159849Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4a4ebba450898adc traceID=9f6f7a7bfbc972926372f175df3f3e2d
Jun 14 16:27:18 minikube dockerd[1014]: time="2024-06-14T16:27:18.457261949Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4a4ebba450898adc traceID=9f6f7a7bfbc972926372f175df3f3e2d
Jun 14 16:27:18 minikube dockerd[1014]: time="2024-06-14T16:27:18.461032356Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4a4ebba450898adc traceID=9f6f7a7bfbc972926372f175df3f3e2d
Jun 14 16:32:28 minikube dockerd[1014]: time="2024-06-14T16:32:28.455336263Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=be29deace1e1e3d7 traceID=852a575a480100435f04734fea75ef00
Jun 14 16:32:28 minikube dockerd[1014]: time="2024-06-14T16:32:28.455417164Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=be29deace1e1e3d7 traceID=852a575a480100435f04734fea75ef00
Jun 14 16:32:28 minikube dockerd[1014]: time="2024-06-14T16:32:28.459003401Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=be29deace1e1e3d7 traceID=852a575a480100435f04734fea75ef00
Jun 14 16:37:41 minikube dockerd[1014]: time="2024-06-14T16:37:41.464784842Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=7de65ac055ab248c traceID=0997118264af32231047ce81011e0316
Jun 14 16:37:41 minikube dockerd[1014]: time="2024-06-14T16:37:41.464984019Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=7de65ac055ab248c traceID=0997118264af32231047ce81011e0316
Jun 14 16:37:41 minikube dockerd[1014]: time="2024-06-14T16:37:41.471187489Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=7de65ac055ab248c traceID=0997118264af32231047ce81011e0316
Jun 14 16:42:46 minikube dockerd[1014]: time="2024-06-14T16:42:46.443443133Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4d4dae8b02c3cb1e traceID=9911b59531d66b060c3891d4389614f7
Jun 14 16:42:46 minikube dockerd[1014]: time="2024-06-14T16:42:46.443495833Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4d4dae8b02c3cb1e traceID=9911b59531d66b060c3891d4389614f7
Jun 14 16:42:46 minikube dockerd[1014]: time="2024-06-14T16:42:46.446780864Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=4d4dae8b02c3cb1e traceID=9911b59531d66b060c3891d4389614f7
Jun 14 16:47:50 minikube dockerd[1014]: time="2024-06-14T16:47:50.443812809Z" level=warning msg="Error getting v2 registry: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=8532d168ea08f7f1 traceID=6aa37cfaf6b9b6ea0a4c86068eae5393
Jun 14 16:47:50 minikube dockerd[1014]: time="2024-06-14T16:47:50.443928511Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=8532d168ea08f7f1 traceID=6aa37cfaf6b9b6ea0a4c86068eae5393
Jun 14 16:47:50 minikube dockerd[1014]: time="2024-06-14T16:47:50.446909563Z" level=error msg="Handler for POST /v1.44/images/create returned error: Get \"https://192.168.49.2:5000/v2/\": dial tcp 192.168.49.2:5000: connect: connection refused" spanID=8532d168ea08f7f1 traceID=6aa37cfaf6b9b6ea0a4c86068eae5393
Jun 14 16:49:57 minikube cri-dockerd[1263]: time="2024-06-14T16:49:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bb76e8345526cda84cbce187af5932e64e0afe413d640e54ba6cd8edbff41d8a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 16:49:58 minikube dockerd[1014]: time="2024-06-14T16:49:58.266401527Z" level=info msg="ignoring event" container=c419e50f5c9a573dc782694abf8a5ef7f977b594ea62eb82fa4c2aade5ce9eeb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:04:47 minikube cri-dockerd[1263]: time="2024-06-14T17:04:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0d02e4ba258ec7cab7138a4c8a8fbe3ee2e80ac4c034b02ac242c4f616c4c12a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 17:04:59 minikube cri-dockerd[1263]: time="2024-06-14T17:04:59Z" level=info msg="Pulling image mysql:latest: fb61b56acac1: Extracting [>                                                  ]   98.3kB/6.712MB"
Jun 14 17:05:09 minikube cri-dockerd[1263]: time="2024-06-14T17:05:09Z" level=info msg="Pulling image mysql:latest: ed90f5355e12: Extracting [==========================>                        ]  34.54MB/64.79MB"
Jun 14 17:05:18 minikube cri-dockerd[1263]: time="2024-06-14T17:05:18Z" level=info msg="Stop pulling image mysql:latest: Status: Downloaded newer image for mysql:latest"
Jun 14 17:14:39 minikube cri-dockerd[1263]: time="2024-06-14T17:14:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/25a7ae73ddae4a892fc5887fdd865f79ad2a806ceec2e763b5b9bff966b607da/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 17:14:40 minikube dockerd[1014]: time="2024-06-14T17:14:40.906346565Z" level=info msg="ignoring event" container=2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:14:41 minikube dockerd[1014]: time="2024-06-14T17:14:41.160795100Z" level=info msg="ignoring event" container=bb76e8345526cda84cbce187af5932e64e0afe413d640e54ba6cd8edbff41d8a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:21:56 minikube cri-dockerd[1263]: time="2024-06-14T17:21:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/feb306987843016ec7d56487fb2feacfc0139da27aa8ac7ca50194f0dc68a1c8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 17:21:58 minikube cri-dockerd[1263]: time="2024-06-14T17:21:58Z" level=info msg="Stop pulling image mysql:latest: Status: Image is up to date for mysql:latest"
Jun 14 17:22:00 minikube dockerd[1014]: time="2024-06-14T17:22:00.361893384Z" level=info msg="ignoring event" container=6c7f6cf79378d98d9a82589e7d263c4a9132b84d58c1d8b38273d6f2db352cde module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:22:00 minikube cri-dockerd[1263]: time="2024-06-14T17:22:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mysql-cc9d9d858-mk9tc_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Jun 14 17:22:00 minikube dockerd[1014]: time="2024-06-14T17:22:00.706735566Z" level=info msg="ignoring event" container=0d02e4ba258ec7cab7138a4c8a8fbe3ee2e80ac4c034b02ac242c4f616c4c12a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:24:10 minikube cri-dockerd[1263]: time="2024-06-14T17:24:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68d10835c60e49c76e599bd8771824d2c10fbb1cf9f640afca62bd2da93e0f4a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 17:24:11 minikube dockerd[1014]: time="2024-06-14T17:24:11.586032843Z" level=info msg="ignoring event" container=68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:24:11 minikube dockerd[1014]: time="2024-06-14T17:24:11.813831794Z" level=info msg="ignoring event" container=25a7ae73ddae4a892fc5887fdd865f79ad2a806ceec2e763b5b9bff966b607da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:26:58 minikube cri-dockerd[1263]: time="2024-06-14T17:26:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95e26fafd2b1ec209e1394518f9e5f45883e64e469af6021101c74224127abb6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 14 17:26:59 minikube cri-dockerd[1263]: time="2024-06-14T17:26:59Z" level=info msg="Stop pulling image phpmyadmin/phpmyadmin:latest: Status: Image is up to date for phpmyadmin/phpmyadmin:latest"
Jun 14 17:27:01 minikube dockerd[1014]: time="2024-06-14T17:27:01.887234539Z" level=info msg="ignoring event" container=c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 14 17:27:02 minikube dockerd[1014]: time="2024-06-14T17:27:02.160447822Z" level=info msg="ignoring event" container=a8ba2f5f5fbfc02c87f2ee3ec5d8a6ab62d537804f7f2694417f9c576ace88ea module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
61e8011418ceb       phpmyadmin/phpmyadmin@sha256:67ba2550fd004399ab0b95b64021a88ea544011e566a9a1995180a3decb6410d                           10 minutes ago      Running             php-deployment            0                   95e26fafd2b1e       php-deployment-65d55bcf88-9mnw7
50956537e43a4       12caf990dadea                                                                                                           13 minutes ago      Running             my-webapp                 0                   68d10835c60e4       my-webapp-66c878f6bb-wqgsb
a87e6f09bb875       mysql@sha256:aa021e164da6aacbefc59ed0b933427e4835636be380f3b6523f4a6c9564e1f0                                           15 minutes ago      Running             mysql                     0                   feb3069878430       mysql-5cdf857f86-7bmn4
edc8238b433af       registry.k8s.io/metrics-server/metrics-server@sha256:db3800085a0957083930c3932b17580eec652cfb6156a05c0f79c7543e80d17a   2 hours ago         Running             metrics-server            0                   0a89cb20dae7e       metrics-server-c59844bb4-bn2mb
7ffdbae0bd401       6e38f40d628db                                                                                                           3 hours ago         Running             storage-provisioner       4                   d6c2d31d538b9       storage-provisioner
11f36cc3fe53b       6e38f40d628db                                                                                                           3 hours ago         Exited              storage-provisioner       3                   d6c2d31d538b9       storage-provisioner
32c786efe3eea       cbb01a7bd410d                                                                                                           3 hours ago         Running             coredns                   2                   d3088d1c93172       coredns-7db6d8ff4d-6cf9t
fb50eaddf9109       a0bf559e280cf                                                                                                           3 hours ago         Running             kube-proxy                2                   a3677a2acec02       kube-proxy-j75p7
7b7f97067637a       259c8277fcbbc                                                                                                           3 hours ago         Running             kube-scheduler            2                   d1b508c3dcfd5       kube-scheduler-minikube
37e129bd163dd       3861cfcd7c04c                                                                                                           3 hours ago         Running             etcd                      2                   423acb896be01       etcd-minikube
ba954df85a305       c7aad43836fa5                                                                                                           3 hours ago         Running             kube-controller-manager   2                   99181fbcac73b       kube-controller-manager-minikube
c4246d90a9f62       c42f13656d0b2                                                                                                           3 hours ago         Running             kube-apiserver            2                   cda31e362bf63       kube-apiserver-minikube
bd14f34008dbd       cbb01a7bd410d                                                                                                           2 days ago          Exited              coredns                   1                   08a4009a2eafd       coredns-7db6d8ff4d-6cf9t
b0810b12f715c       3861cfcd7c04c                                                                                                           2 days ago          Exited              etcd                      1                   8250d0da94a91       etcd-minikube
d73d3fda6bb08       c42f13656d0b2                                                                                                           2 days ago          Exited              kube-apiserver            1                   7b09c32a21262       kube-apiserver-minikube
27cb6cb3f3c10       a0bf559e280cf                                                                                                           2 days ago          Exited              kube-proxy                1                   fc317916c21c9       kube-proxy-j75p7
9d2e8098f6985       c7aad43836fa5                                                                                                           2 days ago          Exited              kube-controller-manager   1                   bdbbfb718330d       kube-controller-manager-minikube
abbe234e31ed2       259c8277fcbbc                                                                                                           2 days ago          Exited              kube-scheduler            1                   acc5a05edd430       kube-scheduler-minikube


==> coredns [32c786efe3ee] <==
[INFO] 10.244.0.7:59820 - 20775 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.0001103s
[INFO] 10.244.0.7:59820 - 35625 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.0001444s
[INFO] 10.244.0.7:39118 - 475 "AAAA IN db. udp 20 false 512" - - 0 6.004600467s
[INFO] 10.244.0.7:39118 - 45029 "A IN db. udp 20 false 512" - - 0 6.004388467s
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:60489->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:39074->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:39118 - 45029 "A IN db. udp 20 false 512" - - 0 4.001707904s
[INFO] 10.244.0.7:39118 - 475 "AAAA IN db. udp 20 false 512" - - 0 4.001762204s
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:33643->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:41533->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:38267 - 43322 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000554301s
[INFO] 10.244.0.7:38267 - 19511 "A IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000696902s
[INFO] 10.244.0.7:47163 - 9430 "AAAA IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.0001163s
[INFO] 10.244.0.7:47163 - 4567 "A IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000259601s
[INFO] 10.244.0.7:52017 - 25899 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000119201s
[INFO] 10.244.0.7:52017 - 64036 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.0001358s
[INFO] 10.244.0.7:48537 - 1401 "AAAA IN db. udp 20 false 512" - - 0 2.001014951s
[INFO] 10.244.0.7:48537 - 31864 "A IN db. udp 20 false 512" - - 0 2.001023751s
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:38241->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:52924->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:48537 - 1401 "AAAA IN db. udp 20 false 512" - - 0 2.000929893s
[INFO] 10.244.0.7:48537 - 31864 "A IN db. udp 20 false 512" - - 0 2.000972993s
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:35598->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:38044->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:46024 - 26129 "A IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000423204s
[INFO] 10.244.0.7:46024 - 27671 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000628806s
[INFO] 10.244.0.7:35192 - 10809 "AAAA IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000132801s
[INFO] 10.244.0.7:35192 - 7992 "A IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000205102s
[INFO] 10.244.0.7:56120 - 48205 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000129801s
[INFO] 10.244.0.7:56120 - 40782 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000206302s
[INFO] 10.244.0.7:40625 - 13570 "AAAA IN db. udp 20 false 512" - - 0 2.000739384s
[INFO] 10.244.0.7:40625 - 58112 "A IN db. udp 20 false 512" - - 0 2.000690984s
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:53391->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:53899->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:40625 - 58112 "A IN db. udp 20 false 512" - - 0 2.001023487s
[INFO] 10.244.0.7:40625 - 13570 "AAAA IN db. udp 20 false 512" - - 0 2.001112487s
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:40297->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:44270->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:52677 - 57916 "A IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000739803s
[INFO] 10.244.0.7:52677 - 64574 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.001141305s
[INFO] 10.244.0.7:60396 - 27503 "AAAA IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000250401s
[INFO] 10.244.0.7:60396 - 365 "A IN db.svc.cluster.local. udp 38 false 512" NXDOMAIN qr,aa,rd 131 0.000382901s
[INFO] 10.244.0.7:60139 - 63504 "AAAA IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000125801s
[INFO] 10.244.0.7:60139 - 36374 "A IN db.cluster.local. udp 34 false 512" NXDOMAIN qr,aa,rd 127 0.000203801s
[INFO] 10.244.0.7:59976 - 63331 "A IN db. udp 20 false 512" - - 0 2.000298387s
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:50682->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:59976 - 48224 "AAAA IN db. udp 20 false 512" - - 0 2.000468587s
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:60264->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:59976 - 63331 "A IN db. udp 20 false 512" - - 0 2.00038811s
[ERROR] plugin/errors: 2 db. A: read udp 10.244.0.4:55826->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.7:59976 - 48224 "AAAA IN db. udp 20 false 512" - - 0 2.00058371s
[ERROR] plugin/errors: 2 db. AAAA: read udp 10.244.0.4:53484->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.13:45396 - 40305 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.0005357s
[INFO] 10.244.0.13:45396 - 58227 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0006037s
[INFO] 10.244.0.13:39525 - 61753 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000205s
[INFO] 10.244.0.13:39525 - 39739 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.0003732s
[INFO] 10.244.0.13:36387 - 19687 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000164701s
[INFO] 10.244.0.13:36387 - 17125 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0002229s
[INFO] 10.244.0.13:56627 - 51999 "AAAA IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.0001173s
[INFO] 10.244.0.13:56627 - 54557 "A IN mysql.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0001814s


==> coredns [bd14f34008db] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:41766 - 17839 "HINFO IN 6290352455275547084.9067227818844704501. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.019270211s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_12T17_15_13_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 12 Jun 2024 15:15:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 14 Jun 2024 17:37:38 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 14 Jun 2024 17:36:13 +0000   Wed, 12 Jun 2024 15:15:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 14 Jun 2024 17:36:13 +0000   Wed, 12 Jun 2024 15:15:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 14 Jun 2024 17:36:13 +0000   Wed, 12 Jun 2024 15:15:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 14 Jun 2024 17:36:13 +0000   Wed, 12 Jun 2024 15:15:12 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075336Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075336Ki
  pods:               110
System Info:
  Machine ID:                 966f9cc9bce94e1b9d290a77a3c7e1b3
  System UUID:                966f9cc9bce94e1b9d290a77a3c7e1b3
  Boot ID:                    ad97ce15-3391-4662-bff8-e05f031a4c32
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     my-webapp-66c878f6bb-wqgsb          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13m
  default                     mysql-5cdf857f86-7bmn4              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15m
  default                     php-deployment-65d55bcf88-9mnw7     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  kube-system                 coredns-7db6d8ff4d-6cf9t            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     2d2h
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         2d2h
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d2h
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d2h
  kube-system                 kube-proxy-j75p7                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d2h
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d2h
  kube-system                 metrics-server-c59844bb4-bn2mb      100m (1%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         137m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d2h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  0 (0%!)(MISSING)
  memory             370Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Jun14 14:53] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.010789] PCI: Fatal: No config space access function found
[  +0.020299] PCI: System does not support PCI
[  +0.127227] kvm: already loaded the other module
[  +1.171324] FS-Cache: Duplicate cookie detected
[  +0.000440] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000528] FS-Cache: O-cookie d=00000000d18c6095{9P.session} n=00000000a0b37108
[  +0.000578] FS-Cache: O-key=[10] '34323934393337343330'
[  +0.000468] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000438] FS-Cache: N-cookie d=00000000d18c6095{9P.session} n=00000000a1d9920e
[  +0.000722] FS-Cache: N-key=[10] '34323934393337343330'
[  +0.002122] FS-Cache: Duplicate cookie detected
[  +0.000667] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000627] FS-Cache: O-cookie d=00000000d18c6095{9P.session} n=00000000a0b37108
[  +0.000806] FS-Cache: O-key=[10] '34323934393337343330'
[  +0.000713] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.002703] FS-Cache: N-cookie d=00000000d18c6095{9P.session} n=0000000037f828ff
[  +0.001687] FS-Cache: N-key=[10] '34323934393337343330'
[  +0.876316] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.019944] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Madrid not found. Is the tzdata package installed?
[  +0.510971] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +3.825515] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001044] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.000887] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.003465] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.003552] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001080] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.052836] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Madrid not found. Is the tzdata package installed?
[  +0.083189] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.442531] netlink: 'init': attribute type 4 has an invalid length.
[  +0.708901] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.


==> etcd [37e129bd163d] <==
{"level":"warn","ts":"2024-06-14T17:05:10.194468Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-14T17:05:09.690316Z","time spent":"502.085412ms","remote":"127.0.0.1:49742","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:13320 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-06-14T17:05:14.159801Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"150.162717ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029857006260008 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:13317 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128029857006260006 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-06-14T17:05:14.160023Z","caller":"traceutil/trace.go:171","msg":"trace[267790570] transaction","detail":"{read_only:false; response_revision:13324; number_of_response:1; }","duration":"279.413675ms","start":"2024-06-14T17:05:13.880596Z","end":"2024-06-14T17:05:14.16001Z","steps":["trace[267790570] 'process raft request'  (duration: 128.849658ms)","trace[267790570] 'compare'  (duration: 149.998618ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:14.211203Z","caller":"traceutil/trace.go:171","msg":"trace[831364637] transaction","detail":"{read_only:false; response_revision:13325; number_of_response:1; }","duration":"123.524968ms","start":"2024-06-14T17:05:14.087654Z","end":"2024-06-14T17:05:14.211179Z","steps":["trace[831364637] 'process raft request'  (duration: 123.236268ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:14.485548Z","caller":"traceutil/trace.go:171","msg":"trace[1594893329] transaction","detail":"{read_only:false; response_revision:13326; number_of_response:1; }","duration":"241.202947ms","start":"2024-06-14T17:05:14.244326Z","end":"2024-06-14T17:05:14.485529Z","steps":["trace[1594893329] 'process raft request'  (duration: 241.033548ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-14T17:05:14.485786Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"233.353662ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-06-14T17:05:14.485985Z","caller":"traceutil/trace.go:171","msg":"trace[274396336] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:13326; }","duration":"233.529362ms","start":"2024-06-14T17:05:14.252388Z","end":"2024-06-14T17:05:14.485918Z","steps":["trace[274396336] 'agreement among raft nodes before linearized reading'  (duration: 233.272162ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:14.485581Z","caller":"traceutil/trace.go:171","msg":"trace[1577410932] linearizableReadLoop","detail":"{readStateIndex:16598; appliedIndex:16597; }","duration":"233.045762ms","start":"2024-06-14T17:05:14.252443Z","end":"2024-06-14T17:05:14.485489Z","steps":["trace[1577410932] 'read index received'  (duration: 232.828563ms)","trace[1577410932] 'applied index is now lower than readState.Index'  (duration: 216.199µs)"],"step_count":2}
{"level":"warn","ts":"2024-06-14T17:05:16.775961Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.704554ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-14T17:05:16.778218Z","caller":"traceutil/trace.go:171","msg":"trace[190924656] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:13328; }","duration":"132.95925ms","start":"2024-06-14T17:05:16.645086Z","end":"2024-06-14T17:05:16.778046Z","steps":["trace[190924656] 'range keys from in-memory index tree'  (duration: 130.666954ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:18.779381Z","caller":"traceutil/trace.go:171","msg":"trace[558874159] transaction","detail":"{read_only:false; response_revision:13329; number_of_response:1; }","duration":"203.388517ms","start":"2024-06-14T17:05:18.575932Z","end":"2024-06-14T17:05:18.779321Z","steps":["trace[558874159] 'process raft request'  (duration: 203.250718ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:19.010689Z","caller":"traceutil/trace.go:171","msg":"trace[1494197545] transaction","detail":"{read_only:false; response_revision:13330; number_of_response:1; }","duration":"154.039611ms","start":"2024-06-14T17:05:18.856634Z","end":"2024-06-14T17:05:19.010673Z","steps":["trace[1494197545] 'process raft request'  (duration: 101.944609ms)","trace[1494197545] 'compare'  (duration: 51.916802ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-14T17:05:19.497028Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"152.152514ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029857006260035 > lease_revoke:<id:70cc90173ea08f00>","response":"size:29"}
{"level":"info","ts":"2024-06-14T17:05:20.91372Z","caller":"traceutil/trace.go:171","msg":"trace[1423038105] transaction","detail":"{read_only:false; response_revision:13332; number_of_response:1; }","duration":"121.405682ms","start":"2024-06-14T17:05:20.792261Z","end":"2024-06-14T17:05:20.913666Z","steps":["trace[1423038105] 'process raft request'  (duration: 120.788581ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:22.349036Z","caller":"traceutil/trace.go:171","msg":"trace[485444623] transaction","detail":"{read_only:false; response_revision:13337; number_of_response:1; }","duration":"156.975306ms","start":"2024-06-14T17:05:22.192044Z","end":"2024-06-14T17:05:22.349019Z","steps":["trace[485444623] 'process raft request'  (duration: 125.093884ms)","trace[485444623] 'compare'  (duration: 31.644122ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:22.349244Z","caller":"traceutil/trace.go:171","msg":"trace[29107341] transaction","detail":"{read_only:false; response_revision:13338; number_of_response:1; }","duration":"154.730804ms","start":"2024-06-14T17:05:22.194484Z","end":"2024-06-14T17:05:22.349215Z","steps":["trace[29107341] 'process raft request'  (duration: 154.480404ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:22.732476Z","caller":"traceutil/trace.go:171","msg":"trace[1420009149] transaction","detail":"{read_only:false; response_revision:13341; number_of_response:1; }","duration":"277.407187ms","start":"2024-06-14T17:05:22.455054Z","end":"2024-06-14T17:05:22.732461Z","steps":["trace[1420009149] 'process raft request'  (duration: 212.976344ms)","trace[1420009149] 'compare'  (duration: 64.317043ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:22.793563Z","caller":"traceutil/trace.go:171","msg":"trace[1120629372] linearizableReadLoop","detail":"{readStateIndex:16615; appliedIndex:16613; }","duration":"154.815204ms","start":"2024-06-14T17:05:22.638733Z","end":"2024-06-14T17:05:22.793548Z","steps":["trace[1120629372] 'read index received'  (duration: 29.42092ms)","trace[1120629372] 'applied index is now lower than readState.Index'  (duration: 125.393584ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:22.793781Z","caller":"traceutil/trace.go:171","msg":"trace[1412154703] transaction","detail":"{read_only:false; response_revision:13342; number_of_response:1; }","duration":"253.957471ms","start":"2024-06-14T17:05:22.539806Z","end":"2024-06-14T17:05:22.793763Z","steps":["trace[1412154703] 'process raft request'  (duration: 253.601071ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-14T17:05:22.793771Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.018104ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/poddisruptionbudgets/\" range_end:\"/registry/poddisruptionbudgets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-14T17:05:22.794044Z","caller":"traceutil/trace.go:171","msg":"trace[1790645690] range","detail":"{range_begin:/registry/poddisruptionbudgets/; range_end:/registry/poddisruptionbudgets0; response_count:0; response_revision:13342; }","duration":"155.394704ms","start":"2024-06-14T17:05:22.638633Z","end":"2024-06-14T17:05:22.794028Z","steps":["trace[1790645690] 'agreement among raft nodes before linearized reading'  (duration: 155.070604ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:22.982526Z","caller":"traceutil/trace.go:171","msg":"trace[1249343983] transaction","detail":"{read_only:false; response_revision:13343; number_of_response:1; }","duration":"244.153464ms","start":"2024-06-14T17:05:22.738322Z","end":"2024-06-14T17:05:22.982476Z","steps":["trace[1249343983] 'process raft request'  (duration: 243.933464ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:23.195734Z","caller":"traceutil/trace.go:171","msg":"trace[1246401581] linearizableReadLoop","detail":"{readStateIndex:16618; appliedIndex:16616; }","duration":"273.698685ms","start":"2024-06-14T17:05:22.922005Z","end":"2024-06-14T17:05:23.195704Z","steps":["trace[1246401581] 'read index received'  (duration: 60.549941ms)","trace[1246401581] 'applied index is now lower than readState.Index'  (duration: 213.147044ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:23.195949Z","caller":"traceutil/trace.go:171","msg":"trace[1978946297] transaction","detail":"{read_only:false; response_revision:13345; number_of_response:1; }","duration":"395.609066ms","start":"2024-06-14T17:05:22.800311Z","end":"2024-06-14T17:05:23.19592Z","steps":["trace[1978946297] 'process raft request'  (duration: 395.270866ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-14T17:05:23.19619Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-14T17:05:22.800298Z","time spent":"395.738566ms","remote":"127.0.0.1:50068","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2837,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/replicasets/default/mysql-f5f8f5446\" mod_revision:13339 > success:<request_put:<key:\"/registry/replicasets/default/mysql-f5f8f5446\" value_size:2784 >> failure:<request_range:<key:\"/registry/replicasets/default/mysql-f5f8f5446\" > >"}
{"level":"info","ts":"2024-06-14T17:05:23.196477Z","caller":"traceutil/trace.go:171","msg":"trace[2143742499] transaction","detail":"{read_only:false; response_revision:13344; number_of_response:1; }","duration":"398.722568ms","start":"2024-06-14T17:05:22.797678Z","end":"2024-06-14T17:05:23.1964Z","steps":["trace[2143742499] 'process raft request'  (duration: 309.909108ms)","trace[2143742499] 'compare'  (duration: 87.617059ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-14T17:05:23.196508Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"274.470085ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-06-14T17:05:23.196629Z","caller":"traceutil/trace.go:171","msg":"trace[441478366] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:13345; }","duration":"274.639185ms","start":"2024-06-14T17:05:22.921951Z","end":"2024-06-14T17:05:23.19659Z","steps":["trace[441478366] 'agreement among raft nodes before linearized reading'  (duration: 274.325785ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-14T17:05:23.196631Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-14T17:05:22.797657Z","time spent":"398.897768ms","remote":"127.0.0.1:49648","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":682,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/mysql-f5f8f5446.17d8edcd0dac0acb\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/mysql-f5f8f5446.17d8edcd0dac0acb\" value_size:607 lease:8128029857006259852 >> failure:<>"}
{"level":"info","ts":"2024-06-14T17:05:23.401064Z","caller":"traceutil/trace.go:171","msg":"trace[1287824471] transaction","detail":"{read_only:false; response_revision:13347; number_of_response:1; }","duration":"186.368026ms","start":"2024-06-14T17:05:23.214669Z","end":"2024-06-14T17:05:23.401037Z","steps":["trace[1287824471] 'process raft request'  (duration: 156.668206ms)","trace[1287824471] 'compare'  (duration: 29.48212ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:23.605721Z","caller":"traceutil/trace.go:171","msg":"trace[128388242] linearizableReadLoop","detail":"{readStateIndex:16621; appliedIndex:16620; }","duration":"126.564785ms","start":"2024-06-14T17:05:23.479136Z","end":"2024-06-14T17:05:23.6057Z","steps":["trace[128388242] 'read index received'  (duration: 39.234226ms)","trace[128388242] 'applied index is now lower than readState.Index'  (duration: 87.329859ms)"],"step_count":2}
{"level":"info","ts":"2024-06-14T17:05:23.605753Z","caller":"traceutil/trace.go:171","msg":"trace[732925474] transaction","detail":"{read_only:false; response_revision:13348; number_of_response:1; }","duration":"199.882435ms","start":"2024-06-14T17:05:23.405853Z","end":"2024-06-14T17:05:23.605735Z","steps":["trace[732925474] 'process raft request'  (duration: 112.609276ms)","trace[732925474] 'compare'  (duration: 87.162859ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-14T17:05:23.605823Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.668585ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-14T17:05:23.60603Z","caller":"traceutil/trace.go:171","msg":"trace[1109384870] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:13348; }","duration":"126.730386ms","start":"2024-06-14T17:05:23.479116Z","end":"2024-06-14T17:05:23.605846Z","steps":["trace[1109384870] 'agreement among raft nodes before linearized reading'  (duration: 126.666986ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:24.007491Z","caller":"traceutil/trace.go:171","msg":"trace[698519642] transaction","detail":"{read_only:false; response_revision:13349; number_of_response:1; }","duration":"139.100293ms","start":"2024-06-14T17:05:23.86837Z","end":"2024-06-14T17:05:24.00747Z","steps":["trace[698519642] 'process raft request'  (duration: 69.027346ms)","trace[698519642] 'compare'  (duration: 69.989147ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-14T17:05:24.238201Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.654473ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/mysql-f5f8f5446-4twl5\" ","response":"range_response_count:1 size:3720"}
{"level":"info","ts":"2024-06-14T17:05:24.23829Z","caller":"traceutil/trace.go:171","msg":"trace[2097575757] range","detail":"{range_begin:/registry/pods/default/mysql-f5f8f5446-4twl5; range_end:; response_count:1; response_revision:13349; }","duration":"108.788074ms","start":"2024-06-14T17:05:24.129493Z","end":"2024-06-14T17:05:24.238281Z","steps":["trace[2097575757] 'range keys from in-memory index tree'  (duration: 108.552073ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:24.393927Z","caller":"traceutil/trace.go:171","msg":"trace[884980125] transaction","detail":"{read_only:false; response_revision:13350; number_of_response:1; }","duration":"145.950098ms","start":"2024-06-14T17:05:24.247946Z","end":"2024-06-14T17:05:24.393896Z","steps":["trace[884980125] 'process raft request'  (duration: 145.727998ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:39.620174Z","caller":"traceutil/trace.go:171","msg":"trace[634102959] transaction","detail":"{read_only:false; response_revision:13365; number_of_response:1; }","duration":"123.054042ms","start":"2024-06-14T17:05:39.497106Z","end":"2024-06-14T17:05:39.62016Z","steps":["trace[634102959] 'process raft request'  (duration: 122.939137ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:05:39.929956Z","caller":"traceutil/trace.go:171","msg":"trace[1519378925] transaction","detail":"{read_only:false; response_revision:13366; number_of_response:1; }","duration":"123.825031ms","start":"2024-06-14T17:05:39.806109Z","end":"2024-06-14T17:05:39.929934Z","steps":["trace[1519378925] 'process raft request'  (duration: 123.667831ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-14T17:05:43.969871Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.609659ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029857006260161 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc90173ea08fc0>","response":"size:41"}
{"level":"info","ts":"2024-06-14T17:05:49.975565Z","caller":"traceutil/trace.go:171","msg":"trace[635822507] transaction","detail":"{read_only:false; response_revision:13373; number_of_response:1; }","duration":"135.233816ms","start":"2024-06-14T17:05:49.840318Z","end":"2024-06-14T17:05:49.975552Z","steps":["trace[635822507] 'process raft request'  (duration: 135.047613ms)"],"step_count":1}
{"level":"info","ts":"2024-06-14T17:10:01.147061Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13314}
{"level":"info","ts":"2024-06-14T17:10:01.152229Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13314,"took":"4.894376ms","hash":1916804861,"current-db-size-bytes":2633728,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1814528,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-06-14T17:10:01.152271Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1916804861,"revision":13314,"compact-revision":13044}
{"level":"info","ts":"2024-06-14T17:15:01.15908Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13573}
{"level":"info","ts":"2024-06-14T17:15:01.166828Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13573,"took":"7.426706ms","hash":1837025561,"current-db-size-bytes":2633728,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1826816,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-06-14T17:15:01.166918Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1837025561,"revision":13573,"compact-revision":13314}
{"level":"info","ts":"2024-06-14T17:20:01.153696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13848}
{"level":"info","ts":"2024-06-14T17:20:01.157466Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":13848,"took":"3.514688ms","hash":1807205855,"current-db-size-bytes":2633728,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1761280,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-06-14T17:20:01.157514Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1807205855,"revision":13848,"compact-revision":13573}
{"level":"info","ts":"2024-06-14T17:25:01.16418Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14090}
{"level":"info","ts":"2024-06-14T17:25:01.168829Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":14090,"took":"4.321483ms","hash":129040906,"current-db-size-bytes":2633728,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1826816,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-06-14T17:25:01.168896Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":129040906,"revision":14090,"compact-revision":13848}
{"level":"info","ts":"2024-06-14T17:30:01.180122Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14401}
{"level":"info","ts":"2024-06-14T17:30:01.189646Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":14401,"took":"8.687585ms","hash":3486210342,"current-db-size-bytes":2633728,"current-db-size":"2.6 MB","current-db-size-in-use-bytes":1978368,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-06-14T17:30:01.189774Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3486210342,"revision":14401,"compact-revision":14090}
{"level":"info","ts":"2024-06-14T17:35:01.188336Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14684}
{"level":"info","ts":"2024-06-14T17:35:01.194292Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":14684,"took":"5.708531ms","hash":3181767283,"current-db-size-bytes":2740224,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1863680,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-06-14T17:35:01.194343Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3181767283,"revision":14684,"compact-revision":14401}


==> etcd [b0810b12f715] <==
{"level":"warn","ts":"2024-06-12T16:23:28.899347Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.461401ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-12T16:23:28.899402Z","caller":"traceutil/trace.go:171","msg":"trace[2129384024] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3734; }","duration":"121.515501ms","start":"2024-06-12T16:23:28.77785Z","end":"2024-06-12T16:23:28.899365Z","steps":["trace[2129384024] 'agreement among raft nodes before linearized reading'  (duration: 121.460701ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:23:40.90586Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.222627ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-12T16:23:40.906048Z","caller":"traceutil/trace.go:171","msg":"trace[341035113] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3743; }","duration":"127.349927ms","start":"2024-06-12T16:23:40.778586Z","end":"2024-06-12T16:23:40.905936Z","steps":["trace[341035113] 'range keys from in-memory index tree'  (duration: 127.077127ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:23:41.357119Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.681185ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029813860136863 > lease_revoke:<id:70cc900d32eaf760>","response":"size:29"}
{"level":"warn","ts":"2024-06-12T16:23:55.079064Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.780471ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-12T16:23:55.079372Z","caller":"traceutil/trace.go:171","msg":"trace[368303988] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3753; }","duration":"136.126973ms","start":"2024-06-12T16:23:54.943231Z","end":"2024-06-12T16:23:55.079358Z","steps":["trace[368303988] 'range keys from in-memory index tree'  (duration: 135.769571ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:23:56.304963Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"178.021416ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029813860136918 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:3746 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128029813860136916 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-06-12T16:23:56.305238Z","caller":"traceutil/trace.go:171","msg":"trace[153311730] transaction","detail":"{read_only:false; response_revision:3754; number_of_response:1; }","duration":"286.311659ms","start":"2024-06-12T16:23:56.018912Z","end":"2024-06-12T16:23:56.305224Z","steps":["trace[153311730] 'process raft request'  (duration: 107.974243ms)","trace[153311730] 'compare'  (duration: 177.760215ms)"],"step_count":2}
{"level":"info","ts":"2024-06-12T16:23:56.759742Z","caller":"traceutil/trace.go:171","msg":"trace[1388088312] transaction","detail":"{read_only:false; response_revision:3755; number_of_response:1; }","duration":"103.977626ms","start":"2024-06-12T16:23:56.655748Z","end":"2024-06-12T16:23:56.759726Z","steps":["trace[1388088312] 'process raft request'  (duration: 103.812226ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:23:56.919969Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"141.448671ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-12T16:23:56.920039Z","caller":"traceutil/trace.go:171","msg":"trace[778328750] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3755; }","duration":"141.564071ms","start":"2024-06-12T16:23:56.778463Z","end":"2024-06-12T16:23:56.920027Z","steps":["trace[778328750] 'range keys from in-memory index tree'  (duration: 141.405271ms)"],"step_count":1}
{"level":"info","ts":"2024-06-12T16:23:57.170875Z","caller":"traceutil/trace.go:171","msg":"trace[1291034492] transaction","detail":"{read_only:false; response_revision:3756; number_of_response:1; }","duration":"247.4052ms","start":"2024-06-12T16:23:56.923454Z","end":"2024-06-12T16:23:57.170859Z","steps":["trace[1291034492] 'process raft request'  (duration: 247.257799ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:24:01.303961Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.457686ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029813860136936 > lease_revoke:<id:70cc900d32eaf7af>","response":"size:29"}
{"level":"info","ts":"2024-06-12T16:24:03.583781Z","caller":"traceutil/trace.go:171","msg":"trace[2082890245] transaction","detail":"{read_only:false; response_revision:3760; number_of_response:1; }","duration":"208.521271ms","start":"2024-06-12T16:24:03.375247Z","end":"2024-06-12T16:24:03.583768Z","steps":["trace[2082890245] 'process raft request'  (duration: 208.40117ms)"],"step_count":1}
{"level":"info","ts":"2024-06-12T16:24:46.212096Z","caller":"traceutil/trace.go:171","msg":"trace[2067697577] transaction","detail":"{read_only:false; response_revision:3794; number_of_response:1; }","duration":"180.77943ms","start":"2024-06-12T16:24:46.031274Z","end":"2024-06-12T16:24:46.212054Z","steps":["trace[2067697577] 'process raft request'  (duration: 88.319359ms)","trace[2067697577] 'compare'  (duration: 92.300671ms)"],"step_count":2}
{"level":"info","ts":"2024-06-12T16:24:47.648312Z","caller":"traceutil/trace.go:171","msg":"trace[991703500] transaction","detail":"{read_only:false; response_revision:3795; number_of_response:1; }","duration":"110.535324ms","start":"2024-06-12T16:24:47.537751Z","end":"2024-06-12T16:24:47.648286Z","steps":["trace[991703500] 'process raft request'  (duration: 110.441924ms)"],"step_count":1}
{"level":"info","ts":"2024-06-12T16:24:52.095578Z","caller":"traceutil/trace.go:171","msg":"trace[176945663] transaction","detail":"{read_only:false; response_revision:3799; number_of_response:1; }","duration":"114.817637ms","start":"2024-06-12T16:24:51.980747Z","end":"2024-06-12T16:24:52.095565Z","steps":["trace[176945663] 'process raft request'  (duration: 114.735937ms)"],"step_count":1}
{"level":"info","ts":"2024-06-12T16:26:01.403921Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3616}
{"level":"info","ts":"2024-06-12T16:26:01.413386Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3616,"took":"8.492408ms","hash":377741105,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1445888,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:26:01.413541Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":377741105,"revision":3616,"compact-revision":3378}
{"level":"warn","ts":"2024-06-12T16:26:23.168978Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"390.098832ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-12T16:26:23.169095Z","caller":"traceutil/trace.go:171","msg":"trace[280796127] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3871; }","duration":"390.243532ms","start":"2024-06-12T16:26:22.778837Z","end":"2024-06-12T16:26:23.16908Z","steps":["trace[280796127] 'range keys from in-memory index tree'  (duration: 389.995532ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:26:23.169137Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"369.29732ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-06-12T16:26:23.169216Z","caller":"traceutil/trace.go:171","msg":"trace[692696903] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:3871; }","duration":"369.39322ms","start":"2024-06-12T16:26:22.799812Z","end":"2024-06-12T16:26:23.169205Z","steps":["trace[692696903] 'range keys from in-memory index tree'  (duration: 369.24462ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-12T16:26:23.169134Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-12T16:26:22.778819Z","time spent":"390.301232ms","remote":"127.0.0.1:37118","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-06-12T16:26:23.169247Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-12T16:26:22.799789Z","time spent":"369.45092ms","remote":"127.0.0.1:45812","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2024-06-12T16:31:01.419875Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3854}
{"level":"info","ts":"2024-06-12T16:31:01.42631Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3854,"took":"6.032309ms","hash":1495142458,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:31:01.426396Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1495142458,"revision":3854,"compact-revision":3616}
{"level":"info","ts":"2024-06-12T16:36:01.433367Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4094}
{"level":"info","ts":"2024-06-12T16:36:01.439402Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4094,"took":"5.652438ms","hash":101884654,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:36:01.439514Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":101884654,"revision":4094,"compact-revision":3854}
{"level":"info","ts":"2024-06-12T16:41:01.443497Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4334}
{"level":"info","ts":"2024-06-12T16:41:01.44904Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4334,"took":"5.130253ms","hash":598729041,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:41:01.449138Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":598729041,"revision":4334,"compact-revision":4094}
{"level":"info","ts":"2024-06-12T16:46:01.450307Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4574}
{"level":"info","ts":"2024-06-12T16:46:01.45418Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4574,"took":"3.602321ms","hash":2725895135,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:46:01.454242Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2725895135,"revision":4574,"compact-revision":4334}
{"level":"info","ts":"2024-06-12T16:51:01.464544Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4813}
{"level":"info","ts":"2024-06-12T16:51:01.473652Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":4813,"took":"8.246147ms","hash":3111522943,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1470464,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-12T16:51:01.473908Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3111522943,"revision":4813,"compact-revision":4574}
{"level":"info","ts":"2024-06-12T16:56:01.474917Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5053}
{"level":"info","ts":"2024-06-12T16:56:01.480049Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5053,"took":"4.675936ms","hash":942755757,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1437696,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T16:56:01.480136Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":942755757,"revision":5053,"compact-revision":4813}
{"level":"info","ts":"2024-06-12T17:01:01.482324Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5294}
{"level":"info","ts":"2024-06-12T17:01:01.485925Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5294,"took":"3.337499ms","hash":355100515,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T17:01:01.485996Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":355100515,"revision":5294,"compact-revision":5053}
{"level":"info","ts":"2024-06-12T17:06:01.49251Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5533}
{"level":"info","ts":"2024-06-12T17:06:01.497455Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5533,"took":"4.600423ms","hash":4089630192,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T17:06:01.497538Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4089630192,"revision":5533,"compact-revision":5294}
{"level":"info","ts":"2024-06-12T17:11:01.505147Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5774}
{"level":"info","ts":"2024-06-12T17:11:01.510512Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":5774,"took":"4.987805ms","hash":1598912570,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1409024,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-12T17:11:01.510632Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1598912570,"revision":5774,"compact-revision":5533}
{"level":"info","ts":"2024-06-12T17:16:01.521404Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6018}
{"level":"info","ts":"2024-06-12T17:16:01.528461Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6018,"took":"6.422712ms","hash":1578561146,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1617920,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-06-12T17:16:01.528551Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1578561146,"revision":6018,"compact-revision":5774}
{"level":"info","ts":"2024-06-12T17:21:01.533053Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6299}
{"level":"info","ts":"2024-06-12T17:21:01.539149Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":6299,"took":"5.802136ms","hash":1691371156,"current-db-size-bytes":1941504,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-12T17:21:01.539233Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1691371156,"revision":6299,"compact-revision":6018}


==> kernel <==
 17:37:45 up  2:44,  0 users,  load average: 1.00, 0.57, 0.43
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [c4246d90a9f6] <==
I0614 14:55:02.143903       1 autoregister_controller.go:141] Starting autoregister controller
I0614 14:55:02.143921       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0614 14:55:02.143937       1 cache.go:39] Caches are synced for autoregister controller
I0614 14:55:02.147889       1 policy_source.go:224] refreshing policies
I0614 14:55:02.157478       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0614 14:55:02.164325       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0614 14:55:02.257578       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0614 14:55:02.978778       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
E0614 14:55:12.398792       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 81b8024e-57a9-43ee-b3de-c7644831833c, UID in object meta: "
I0614 14:55:15.301669       1 controller.go:615] quota admission added evaluator for: endpoints
I0614 14:55:15.351744       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0614 15:20:23.860339       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0614 15:20:23.926080       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:23.926157       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0614 15:20:23.926208       1 handler_proxy.go:137] error resolving kube-system/metrics-server: service "metrics-server" not found
I0614 15:20:23.929846       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0614 15:20:24.028658       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0614 15:20:24.093383       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0614 15:20:24.694330       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
W0614 15:20:24.860082       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:24.860149       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0614 15:20:24.860215       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0614 15:20:24.860149       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:24.860319       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0614 15:20:24.861318       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0614 15:20:25.637927       1 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.108.150.164"}
W0614 15:20:25.885305       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:25.885361       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0614 15:20:26.083509       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:26.083601       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
W0614 15:20:26.886087       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:26.886142       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0614 15:20:26.886155       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0614 15:20:26.886157       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:20:26.886198       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0614 15:20:26.887460       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0614 15:21:26.887413       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:21:26.887660       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0614 15:21:26.887693       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0614 15:21:26.888214       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:21:26.888530       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0614 15:21:26.890025       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0614 15:21:45.180213       1 handler_proxy.go:93] no RequestInfo found in the context
E0614 15:21:45.180289       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0614 15:21:45.180455       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.150.164:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.150.164:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.150.164:443: connect: connection refused
E0614 15:21:45.182082       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.150.164:443/apis/metrics.k8s.io/v1beta1: Get "https://10.108.150.164:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.108.150.164:443: connect: connection refused
I0614 15:21:45.217631       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0614 15:51:09.893839       1 alloc.go:330] "allocated clusterIPs" service="default/my-webapp" clusterIPs={"IPv4":"10.100.31.190"}
I0614 16:27:55.554209       1 alloc.go:330] "allocated clusterIPs" service="default/php-service" clusterIPs={"IPv4":"10.101.50.8"}
I0614 17:05:10.195886       1 trace.go:236] Trace[1627806901]: "Update" accept:application/json, */*,audit-id:eb1995ec-1979-4697-b48f-3db4c03960a9,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (14-Jun-2024 17:05:09.688) (total time: 506ms):
Trace[1627806901]: ["GuaranteedUpdate etcd3" audit-id:eb1995ec-1979-4697-b48f-3db4c03960a9,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 506ms (17:05:09.689)
Trace[1627806901]:  ---"Txn call completed" 505ms (17:05:10.195)]
Trace[1627806901]: [506.638799ms] [506.638799ms] END


==> kube-apiserver [d73d3fda6bb0] <==
I0612 16:06:04.450389       1 naming_controller.go:291] Starting NamingConditionController
I0612 16:06:04.450408       1 establishing_controller.go:76] Starting EstablishingController
I0612 16:06:04.450432       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0612 16:06:04.450448       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0612 16:06:04.450470       1 crd_finalizer.go:266] Starting CRDFinalizer
I0612 16:06:04.450561       1 controller.go:78] Starting OpenAPI AggregationController
I0612 16:06:04.450638       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0612 16:06:04.450839       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0612 16:06:04.450888       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0612 16:06:04.463712       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0612 16:06:04.463733       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0612 16:06:04.463768       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0612 16:06:04.463837       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0612 16:06:04.559894       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0612 16:06:04.559916       1 policy_source.go:224] refreshing policies
I0612 16:06:04.560911       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0612 16:06:04.561350       1 shared_informer.go:320] Caches are synced for configmaps
I0612 16:06:04.564166       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0612 16:06:04.565366       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0612 16:06:04.641834       1 shared_informer.go:320] Caches are synced for node_authorizer
I0612 16:06:04.651033       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0612 16:06:04.655397       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0612 16:06:04.655422       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0612 16:06:04.655522       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0612 16:06:04.655540       1 aggregator.go:165] initial CRD sync complete...
I0612 16:06:04.655546       1 autoregister_controller.go:141] Starting autoregister controller
I0612 16:06:04.655551       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0612 16:06:04.655557       1 cache.go:39] Caches are synced for autoregister controller
I0612 16:06:04.658415       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0612 16:06:04.750320       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0612 16:06:05.453383       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0612 16:06:05.561393       1 controller.go:615] quota admission added evaluator for: endpoints
I0612 16:06:17.377689       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0612 16:20:16.794702       1 trace.go:236] Trace[1045543751]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (12-Jun-2024 16:20:15.921) (total time: 872ms):
Trace[1045543751]: ---"Transaction prepared" 313ms (16:20:16.236)
Trace[1045543751]: ---"Txn call completed" 557ms (16:20:16.794)
Trace[1045543751]: [872.389233ms] [872.389233ms] END
I0612 16:20:16.794705       1 trace.go:236] Trace[2111971600]: "Update" accept:application/json, */*,audit-id:aa34c239-8ded-40e3-91d9-a8b99a44f88e,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (12-Jun-2024 16:20:16.238) (total time: 555ms):
Trace[2111971600]: ["GuaranteedUpdate etcd3" audit-id:aa34c239-8ded-40e3-91d9-a8b99a44f88e,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 555ms (16:20:16.239)
Trace[2111971600]:  ---"Txn call completed" 554ms (16:20:16.794)]
Trace[2111971600]: [555.924524ms] [555.924524ms] END
I0612 16:20:26.458396       1 trace.go:236] Trace[1904573304]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (12-Jun-2024 16:20:25.922) (total time: 535ms):
Trace[1904573304]: ---"Transaction prepared" 111ms (16:20:26.035)
Trace[1904573304]: ---"Txn call completed" 422ms (16:20:26.458)
Trace[1904573304]: [535.74596ms] [535.74596ms] END
I0612 16:21:46.872791       1 trace.go:236] Trace[138487121]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (12-Jun-2024 16:21:45.926) (total time: 945ms):
Trace[138487121]: ---"initial value restored" 71ms (16:21:45.998)
Trace[138487121]: ---"Transaction prepared" 286ms (16:21:46.285)
Trace[138487121]: ---"Txn call completed" 587ms (16:21:46.872)
Trace[138487121]: [945.902007ms] [945.902007ms] END
I0612 16:21:47.598631       1 trace.go:236] Trace[391701918]: "Get" accept:application/json, */*,audit-id:64f58662-41d6-498b-a53b-f56b6e14efa0,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (12-Jun-2024 16:21:46.980) (total time: 617ms):
Trace[391701918]: ---"About to write a response" 617ms (16:21:47.598)
Trace[391701918]: [617.984858ms] [617.984858ms] END
I0612 16:22:07.027028       1 trace.go:236] Trace[819781902]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:a7715e06-96ca-436b-b231-185c2ae48dcb,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (12-Jun-2024 16:22:06.493) (total time: 533ms):
Trace[819781902]: ["GuaranteedUpdate etcd3" audit-id:a7715e06-96ca-436b-b231-185c2ae48dcb,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 533ms (16:22:06.493)
Trace[819781902]:  ---"Txn call completed" 532ms (16:22:07.026)]
Trace[819781902]: [533.772145ms] [533.772145ms] END
I0612 17:11:22.367398       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0612 17:11:22.397136       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0612 17:12:19.834490       1 alloc.go:330] "allocated clusterIPs" service="default/mysql" clusterIPs={"IPv4":"10.99.19.10"}


==> kube-controller-manager [9d2e8098f698] <==
I0612 16:06:17.124709       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0612 16:06:17.133942       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0612 16:06:17.134882       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0612 16:06:17.143407       1 shared_informer.go:320] Caches are synced for node
I0612 16:06:17.143497       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0612 16:06:17.143519       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0612 16:06:17.143527       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0612 16:06:17.143535       1 shared_informer.go:320] Caches are synced for cidrallocator
I0612 16:06:17.146838       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0612 16:06:17.157808       1 shared_informer.go:320] Caches are synced for HPA
I0612 16:06:17.167240       1 shared_informer.go:320] Caches are synced for persistent volume
I0612 16:06:17.170028       1 shared_informer.go:320] Caches are synced for attach detach
I0612 16:06:17.170523       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0612 16:06:17.170942       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="259.1µs"
I0612 16:06:17.171442       1 shared_informer.go:320] Caches are synced for ReplicationController
I0612 16:06:17.185468       1 shared_informer.go:320] Caches are synced for TTL
I0612 16:06:17.188890       1 shared_informer.go:320] Caches are synced for expand
I0612 16:06:17.188953       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0612 16:06:17.190270       1 shared_informer.go:320] Caches are synced for PVC protection
I0612 16:06:17.192921       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0612 16:06:17.193013       1 shared_informer.go:320] Caches are synced for ephemeral
I0612 16:06:17.195384       1 shared_informer.go:320] Caches are synced for crt configmap
I0612 16:06:17.196611       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0612 16:06:17.196710       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0612 16:06:17.197794       1 shared_informer.go:320] Caches are synced for TTL after finished
I0612 16:06:17.197865       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0612 16:06:17.197878       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0612 16:06:17.202331       1 shared_informer.go:320] Caches are synced for deployment
I0612 16:06:17.202408       1 shared_informer.go:320] Caches are synced for service account
I0612 16:06:17.204655       1 shared_informer.go:320] Caches are synced for GC
I0612 16:06:17.204736       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0612 16:06:17.206993       1 shared_informer.go:320] Caches are synced for job
I0612 16:06:17.209299       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0612 16:06:17.210867       1 shared_informer.go:320] Caches are synced for taint
I0612 16:06:17.210928       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0612 16:06:17.210982       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0612 16:06:17.211060       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0612 16:06:17.212355       1 shared_informer.go:320] Caches are synced for PV protection
I0612 16:06:17.214790       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0612 16:06:17.217737       1 shared_informer.go:320] Caches are synced for namespace
I0612 16:06:17.221067       1 shared_informer.go:320] Caches are synced for disruption
I0612 16:06:17.221083       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0612 16:06:17.250008       1 shared_informer.go:320] Caches are synced for endpoint
I0612 16:06:17.300508       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0612 16:06:17.359601       1 shared_informer.go:320] Caches are synced for stateful set
I0612 16:06:17.361054       1 shared_informer.go:320] Caches are synced for cronjob
I0612 16:06:17.386459       1 shared_informer.go:320] Caches are synced for resource quota
I0612 16:06:17.422016       1 shared_informer.go:320] Caches are synced for daemon sets
I0612 16:06:17.424981       1 shared_informer.go:320] Caches are synced for resource quota
I0612 16:06:17.835398       1 shared_informer.go:320] Caches are synced for garbage collector
I0612 16:06:17.920399       1 shared_informer.go:320] Caches are synced for garbage collector
I0612 16:06:17.920441       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0612 17:06:16.558031       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-l278c" approvedExpiration="1h0m0s"
I0612 17:11:22.432009       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="30.989265ms"
I0612 17:11:22.439206       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="7.103292ms"
I0612 17:11:22.439339       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="49.6µs"
I0612 17:11:22.461430       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="41.5µs"
I0612 17:13:45.645101       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="92.801µs"
I0612 17:13:47.667093       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="91µs"
I0612 17:13:47.686816       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-7bf8d567b5" duration="77.3µs"


==> kube-controller-manager [ba954df85a30] <==
I0614 17:05:23.202465       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="754.890208ms"
I0614 17:05:23.402509       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="199.877135ms"
I0614 17:05:23.402653       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="55µs"
I0614 17:05:24.395454       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="41.1µs"
I0614 17:05:24.418798       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="38.1µs"
I0614 17:05:24.423524       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-f5f8f5446" duration="40.5µs"
I0614 17:14:38.520849       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="18.675221ms"
I0614 17:14:38.526268       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="5.345806ms"
I0614 17:14:38.526345       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="38.6µs"
I0614 17:14:38.538622       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="40.7µs"
I0614 17:14:39.826644       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="7.401309ms"
I0614 17:14:39.826771       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="67.8µs"
I0614 17:14:39.855555       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="17.86002ms"
I0614 17:14:39.860896       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="5.297306ms"
I0614 17:14:39.861028       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="33.4µs"
I0614 17:14:41.222608       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="53.7µs"
I0614 17:14:41.852749       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="59.299µs"
I0614 17:14:41.864113       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="49.7µs"
I0614 17:14:41.869298       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-76df76ccb8" duration="41.8µs"
I0614 17:21:55.912806       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="18.432024ms"
I0614 17:21:55.922371       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="9.50126ms"
I0614 17:21:55.922464       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="35.5µs"
I0614 17:21:55.922515       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="19.7µs"
I0614 17:21:55.936200       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="65.2µs"
I0614 17:21:58.644044       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="12.769248ms"
I0614 17:21:58.644179       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-5cdf857f86" duration="53.6µs"
I0614 17:21:58.717842       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="61.732046ms"
I0614 17:21:58.726630       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="8.677064ms"
I0614 17:21:58.726699       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="37.5µs"
I0614 17:21:58.726746       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="27µs"
I0614 17:22:00.762692       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="41.3µs"
I0614 17:22:01.742962       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="53.9µs"
I0614 17:22:01.750078       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mysql-cc9d9d858" duration="117.2µs"
I0614 17:24:09.690014       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="17.488066ms"
I0614 17:24:09.697937       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="7.817884ms"
I0614 17:24:09.712568       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="14.581071ms"
I0614 17:24:09.712708       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="37.1µs"
I0614 17:24:10.499871       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="7.974984ms"
I0614 17:24:10.499990       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-66c878f6bb" duration="43.1µs"
I0614 17:24:10.532808       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="19.003963ms"
I0614 17:24:10.540904       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="8.029984ms"
I0614 17:24:10.541009       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="34.7µs"
I0614 17:24:11.880880       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="57.5µs"
I0614 17:24:12.527217       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="93.9µs"
I0614 17:24:12.540015       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="62.7µs"
I0614 17:24:12.544781       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-webapp-7df99c96c5" duration="38.4µs"
I0614 17:26:57.747303       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="16.586651ms"
I0614 17:26:57.767059       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="19.698041ms"
I0614 17:26:57.767149       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="37.3µs"
I0614 17:26:57.767206       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="29.7µs"
I0614 17:27:00.787693       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="8.953173ms"
I0614 17:27:00.791875       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-65d55bcf88" duration="41.3µs"
I0614 17:27:00.819693       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="21.445936ms"
I0614 17:27:00.825711       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="5.928082ms"
I0614 17:27:00.825836       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="30.5µs"
I0614 17:27:00.825937       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="23.099µs"
I0614 17:27:02.223414       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="92.6µs"
I0614 17:27:02.828550       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="37.7µs"
I0614 17:27:02.838860       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="41.9µs"
I0614 17:27:02.841372       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/php-deployment-548b64bcf7" duration="44.8µs"


==> kube-proxy [27cb6cb3f3c1] <==
I0612 16:05:57.867406       1 server_linux.go:69] "Using iptables proxy"
E0612 16:05:57.943555       1 server.go:1051] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
I0612 16:06:04.665365       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0612 16:06:04.854529       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0612 16:06:04.854648       1 server_linux.go:165] "Using iptables Proxier"
I0612 16:06:04.858224       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0612 16:06:04.858282       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0612 16:06:04.858312       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0612 16:06:04.858732       1 server.go:872] "Version info" version="v1.30.0"
I0612 16:06:04.858777       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0612 16:06:04.859967       1 config.go:319] "Starting node config controller"
I0612 16:06:04.859975       1 config.go:192] "Starting service config controller"
I0612 16:06:04.859985       1 shared_informer.go:313] Waiting for caches to sync for node config
I0612 16:06:04.859991       1 shared_informer.go:313] Waiting for caches to sync for service config
I0612 16:06:04.860151       1 config.go:101] "Starting endpoint slice config controller"
I0612 16:06:04.860178       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0612 16:06:04.961063       1 shared_informer.go:320] Caches are synced for service config
I0612 16:06:04.961117       1 shared_informer.go:320] Caches are synced for node config
I0612 16:06:04.961091       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [fb50eaddf910] <==
I0614 14:55:06.435807       1 server_linux.go:69] "Using iptables proxy"
I0614 14:55:06.499622       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0614 14:55:06.592800       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0614 14:55:06.592852       1 server_linux.go:165] "Using iptables Proxier"
I0614 14:55:06.595305       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0614 14:55:06.595340       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0614 14:55:06.595379       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0614 14:55:06.595904       1 server.go:872] "Version info" version="v1.30.0"
I0614 14:55:06.595990       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0614 14:55:06.600313       1 config.go:192] "Starting service config controller"
I0614 14:55:06.601789       1 config.go:101] "Starting endpoint slice config controller"
I0614 14:55:06.601931       1 config.go:319] "Starting node config controller"
I0614 14:55:06.602398       1 shared_informer.go:313] Waiting for caches to sync for service config
I0614 14:55:06.602442       1 shared_informer.go:313] Waiting for caches to sync for node config
I0614 14:55:06.602398       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0614 14:55:06.703154       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0614 14:55:06.703233       1 shared_informer.go:320] Caches are synced for service config
I0614 14:55:06.703315       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [7b7f97067637] <==
I0614 14:55:00.167622       1 serving.go:380] Generated self-signed cert in-memory
W0614 14:55:02.055256       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0614 14:55:02.055365       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0614 14:55:02.055382       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0614 14:55:02.055392       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0614 14:55:02.256223       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0614 14:55:02.256402       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0614 14:55:02.264928       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0614 14:55:02.265067       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0614 14:55:02.266318       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0614 14:55:02.272573       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0614 14:55:02.539016       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [abbe234e31ed] <==
I0612 16:06:02.768569       1 serving.go:380] Generated self-signed cert in-memory
W0612 16:06:04.541936       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0612 16:06:04.541968       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0612 16:06:04.541980       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0612 16:06:04.541989       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0612 16:06:04.644981       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0612 16:06:04.645008       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0612 16:06:04.650270       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0612 16:06:04.650366       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0612 16:06:04.650663       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0612 16:06:04.650838       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0612 16:06:04.751209       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 14 17:05:22 minikube kubelet[1483]: E0614 17:05:22.796097    1483 pod_workers.go:1298] "Error syncing pod, skipping" err="unmounted volumes=[mysql-provisioning], unattached volumes=[], failed to process volumes=[]: context canceled" pod="default/mysql-f5f8f5446-4twl5" podUID="0ace3da1-6d78-40e7-b69e-27236ffcb24b"
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.319127    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mv6p6\" (UniqueName: \"kubernetes.io/projected/0ace3da1-6d78-40e7-b69e-27236ffcb24b-kube-api-access-mv6p6\") pod \"0ace3da1-6d78-40e7-b69e-27236ffcb24b\" (UID: \"0ace3da1-6d78-40e7-b69e-27236ffcb24b\") "
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.319425    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"mysql-data\" (UniqueName: \"kubernetes.io/host-path/0ace3da1-6d78-40e7-b69e-27236ffcb24b-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\") pod \"0ace3da1-6d78-40e7-b69e-27236ffcb24b\" (UID: \"0ace3da1-6d78-40e7-b69e-27236ffcb24b\") "
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.319596    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/0ace3da1-6d78-40e7-b69e-27236ffcb24b-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a" (OuterVolumeSpecName: "mysql-data") pod "0ace3da1-6d78-40e7-b69e-27236ffcb24b" (UID: "0ace3da1-6d78-40e7-b69e-27236ffcb24b"). InnerVolumeSpecName "pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.321110    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/0ace3da1-6d78-40e7-b69e-27236ffcb24b-kube-api-access-mv6p6" (OuterVolumeSpecName: "kube-api-access-mv6p6") pod "0ace3da1-6d78-40e7-b69e-27236ffcb24b" (UID: "0ace3da1-6d78-40e7-b69e-27236ffcb24b"). InnerVolumeSpecName "kube-api-access-mv6p6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.419908    1483 reconciler_common.go:289] "Volume detached for volume \"pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\" (UniqueName: \"kubernetes.io/host-path/0ace3da1-6d78-40e7-b69e-27236ffcb24b-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\") on node \"minikube\" DevicePath \"\""
Jun 14 17:05:23 minikube kubelet[1483]: I0614 17:05:23.419988    1483 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-mv6p6\" (UniqueName: \"kubernetes.io/projected/0ace3da1-6d78-40e7-b69e-27236ffcb24b-kube-api-access-mv6p6\") on node \"minikube\" DevicePath \"\""
Jun 14 17:05:24 minikube kubelet[1483]: I0614 17:05:24.527091    1483 reconciler_common.go:289] "Volume detached for volume \"mysql-provisioning\" (UniqueName: \"kubernetes.io/configmap/0ace3da1-6d78-40e7-b69e-27236ffcb24b-mysql-provisioning\") on node \"minikube\" DevicePath \"\""
Jun 14 17:05:25 minikube kubelet[1483]: I0614 17:05:25.439620    1483 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="0ace3da1-6d78-40e7-b69e-27236ffcb24b" path="/var/lib/kubelet/pods/0ace3da1-6d78-40e7-b69e-27236ffcb24b/volumes"
Jun 14 17:14:38 minikube kubelet[1483]: I0614 17:14:38.521400    1483 topology_manager.go:215] "Topology Admit Handler" podUID="79a9cac2-29d3-4916-a778-8b68a90ce75a" podNamespace="default" podName="my-webapp-7df99c96c5-q4gds"
Jun 14 17:14:38 minikube kubelet[1483]: I0614 17:14:38.633134    1483 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dfjtf\" (UniqueName: \"kubernetes.io/projected/79a9cac2-29d3-4916-a778-8b68a90ce75a-kube-api-access-dfjtf\") pod \"my-webapp-7df99c96c5-q4gds\" (UID: \"79a9cac2-29d3-4916-a778-8b68a90ce75a\") " pod="default/my-webapp-7df99c96c5-q4gds"
Jun 14 17:14:39 minikube kubelet[1483]: I0614 17:14:39.844454    1483 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/my-webapp-7df99c96c5-q4gds" podStartSLOduration=1.844430432 podStartE2EDuration="1.844430432s" podCreationTimestamp="2024-06-14 17:14:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-14 17:14:39.819119103 +0000 UTC m=+8385.014288795" watchObservedRunningTime="2024-06-14 17:14:39.844430432 +0000 UTC m=+8385.039600224"
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.354160    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-qqw65\" (UniqueName: \"kubernetes.io/projected/d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c-kube-api-access-qqw65\") pod \"d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c\" (UID: \"d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c\") "
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.356761    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c-kube-api-access-qqw65" (OuterVolumeSpecName: "kube-api-access-qqw65") pod "d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c" (UID: "d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c"). InnerVolumeSpecName "kube-api-access-qqw65". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.454843    1483 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-qqw65\" (UniqueName: \"kubernetes.io/projected/d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c-kube-api-access-qqw65\") on node \"minikube\" DevicePath \"\""
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.842559    1483 scope.go:117] "RemoveContainer" containerID="2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24"
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.867364    1483 scope.go:117] "RemoveContainer" containerID="2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24"
Jun 14 17:14:41 minikube kubelet[1483]: E0614 17:14:41.868905    1483 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24" containerID="2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24"
Jun 14 17:14:41 minikube kubelet[1483]: I0614 17:14:41.868955    1483 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24"} err="failed to get container status \"2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24\": rpc error: code = Unknown desc = Error response from daemon: No such container: 2ebefdbb26e530acc6916d6be9203f6732c678bf949fd3f6bd4eb59ce7379b24"
Jun 14 17:14:43 minikube kubelet[1483]: I0614 17:14:43.438866    1483 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c" path="/var/lib/kubelet/pods/d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c/volumes"
Jun 14 17:21:55 minikube kubelet[1483]: I0614 17:21:55.913814    1483 topology_manager.go:215] "Topology Admit Handler" podUID="8ed6485a-55a8-46d1-be25-f6aeee943f21" podNamespace="default" podName="mysql-5cdf857f86-7bmn4"
Jun 14 17:21:55 minikube kubelet[1483]: E0614 17:21:55.914448    1483 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c" containerName="my-webapp"
Jun 14 17:21:55 minikube kubelet[1483]: I0614 17:21:55.914795    1483 memory_manager.go:354] "RemoveStaleState removing state" podUID="d7c07169-6d40-4ed2-9b6a-80d0cb45ed1c" containerName="my-webapp"
Jun 14 17:21:55 minikube kubelet[1483]: I0614 17:21:55.990782    1483 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fwzbx\" (UniqueName: \"kubernetes.io/projected/8ed6485a-55a8-46d1-be25-f6aeee943f21-kube-api-access-fwzbx\") pod \"mysql-5cdf857f86-7bmn4\" (UID: \"8ed6485a-55a8-46d1-be25-f6aeee943f21\") " pod="default/mysql-5cdf857f86-7bmn4"
Jun 14 17:21:55 minikube kubelet[1483]: I0614 17:21:55.990897    1483 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\" (UniqueName: \"kubernetes.io/host-path/8ed6485a-55a8-46d1-be25-f6aeee943f21-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\") pod \"mysql-5cdf857f86-7bmn4\" (UID: \"8ed6485a-55a8-46d1-be25-f6aeee943f21\") " pod="default/mysql-5cdf857f86-7bmn4"
Jun 14 17:21:58 minikube kubelet[1483]: I0614 17:21:58.631356    1483 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mysql-5cdf857f86-7bmn4" podStartSLOduration=2.015766843 podStartE2EDuration="3.6313334s" podCreationTimestamp="2024-06-14 17:21:55 +0000 UTC" firstStartedPulling="2024-06-14 17:21:56.586173309 +0000 UTC m=+8821.794821134" lastFinishedPulling="2024-06-14 17:21:58.201739866 +0000 UTC m=+8823.410387691" observedRunningTime="2024-06-14 17:21:58.630835602 +0000 UTC m=+8823.839483427" watchObservedRunningTime="2024-06-14 17:21:58.6313334 +0000 UTC m=+8823.839981225"
Jun 14 17:22:00 minikube kubelet[1483]: I0614 17:22:00.930369    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2gpb6\" (UniqueName: \"kubernetes.io/projected/720cdd93-0933-42e5-afc9-1fb747d08d06-kube-api-access-2gpb6\") pod \"720cdd93-0933-42e5-afc9-1fb747d08d06\" (UID: \"720cdd93-0933-42e5-afc9-1fb747d08d06\") "
Jun 14 17:22:00 minikube kubelet[1483]: I0614 17:22:00.930465    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"mysql-data\" (UniqueName: \"kubernetes.io/host-path/720cdd93-0933-42e5-afc9-1fb747d08d06-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\") pod \"720cdd93-0933-42e5-afc9-1fb747d08d06\" (UID: \"720cdd93-0933-42e5-afc9-1fb747d08d06\") "
Jun 14 17:22:00 minikube kubelet[1483]: I0614 17:22:00.930540    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/720cdd93-0933-42e5-afc9-1fb747d08d06-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a" (OuterVolumeSpecName: "mysql-data") pod "720cdd93-0933-42e5-afc9-1fb747d08d06" (UID: "720cdd93-0933-42e5-afc9-1fb747d08d06"). InnerVolumeSpecName "pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Jun 14 17:22:00 minikube kubelet[1483]: I0614 17:22:00.933080    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/720cdd93-0933-42e5-afc9-1fb747d08d06-kube-api-access-2gpb6" (OuterVolumeSpecName: "kube-api-access-2gpb6") pod "720cdd93-0933-42e5-afc9-1fb747d08d06" (UID: "720cdd93-0933-42e5-afc9-1fb747d08d06"). InnerVolumeSpecName "kube-api-access-2gpb6". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 14 17:22:01 minikube kubelet[1483]: I0614 17:22:01.031702    1483 reconciler_common.go:289] "Volume detached for volume \"pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\" (UniqueName: \"kubernetes.io/host-path/720cdd93-0933-42e5-afc9-1fb747d08d06-pvc-71272cd2-48b1-45ad-92d3-ac053c49e83a\") on node \"minikube\" DevicePath \"\""
Jun 14 17:22:01 minikube kubelet[1483]: I0614 17:22:01.031806    1483 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-2gpb6\" (UniqueName: \"kubernetes.io/projected/720cdd93-0933-42e5-afc9-1fb747d08d06-kube-api-access-2gpb6\") on node \"minikube\" DevicePath \"\""
Jun 14 17:22:01 minikube kubelet[1483]: I0614 17:22:01.726667    1483 scope.go:117] "RemoveContainer" containerID="6c7f6cf79378d98d9a82589e7d263c4a9132b84d58c1d8b38273d6f2db352cde"
Jun 14 17:22:03 minikube kubelet[1483]: I0614 17:22:03.419280    1483 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="720cdd93-0933-42e5-afc9-1fb747d08d06" path="/var/lib/kubelet/pods/720cdd93-0933-42e5-afc9-1fb747d08d06/volumes"
Jun 14 17:24:09 minikube kubelet[1483]: I0614 17:24:09.690625    1483 topology_manager.go:215] "Topology Admit Handler" podUID="d2d98642-0572-44c2-aeaa-73e91f2722b7" podNamespace="default" podName="my-webapp-66c878f6bb-wqgsb"
Jun 14 17:24:09 minikube kubelet[1483]: E0614 17:24:09.690810    1483 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="720cdd93-0933-42e5-afc9-1fb747d08d06" containerName="mysql"
Jun 14 17:24:09 minikube kubelet[1483]: I0614 17:24:09.690894    1483 memory_manager.go:354] "RemoveStaleState removing state" podUID="720cdd93-0933-42e5-afc9-1fb747d08d06" containerName="mysql"
Jun 14 17:24:09 minikube kubelet[1483]: I0614 17:24:09.824436    1483 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5g22v\" (UniqueName: \"kubernetes.io/projected/d2d98642-0572-44c2-aeaa-73e91f2722b7-kube-api-access-5g22v\") pod \"my-webapp-66c878f6bb-wqgsb\" (UID: \"d2d98642-0572-44c2-aeaa-73e91f2722b7\") " pod="default/my-webapp-66c878f6bb-wqgsb"
Jun 14 17:24:10 minikube kubelet[1483]: I0614 17:24:10.523258    1483 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/my-webapp-66c878f6bb-wqgsb" podStartSLOduration=1.5232407380000001 podStartE2EDuration="1.523240738s" podCreationTimestamp="2024-06-14 17:24:09 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-14 17:24:10.492032499 +0000 UTC m=+8955.700733847" watchObservedRunningTime="2024-06-14 17:24:10.523240738 +0000 UTC m=+8955.731942086"
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.041973    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dfjtf\" (UniqueName: \"kubernetes.io/projected/79a9cac2-29d3-4916-a778-8b68a90ce75a-kube-api-access-dfjtf\") pod \"79a9cac2-29d3-4916-a778-8b68a90ce75a\" (UID: \"79a9cac2-29d3-4916-a778-8b68a90ce75a\") "
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.044419    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/79a9cac2-29d3-4916-a778-8b68a90ce75a-kube-api-access-dfjtf" (OuterVolumeSpecName: "kube-api-access-dfjtf") pod "79a9cac2-29d3-4916-a778-8b68a90ce75a" (UID: "79a9cac2-29d3-4916-a778-8b68a90ce75a"). InnerVolumeSpecName "kube-api-access-dfjtf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.142873    1483 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-dfjtf\" (UniqueName: \"kubernetes.io/projected/79a9cac2-29d3-4916-a778-8b68a90ce75a-kube-api-access-dfjtf\") on node \"minikube\" DevicePath \"\""
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.512943    1483 scope.go:117] "RemoveContainer" containerID="68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c"
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.541801    1483 scope.go:117] "RemoveContainer" containerID="68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c"
Jun 14 17:24:12 minikube kubelet[1483]: E0614 17:24:12.543277    1483 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c" containerID="68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c"
Jun 14 17:24:12 minikube kubelet[1483]: I0614 17:24:12.543371    1483 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c"} err="failed to get container status \"68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c\": rpc error: code = Unknown desc = Error response from daemon: No such container: 68e1fe0e926a70a08a38be2a469861740f8d7c93a8a21301cde292c3c5ecbe1c"
Jun 14 17:24:13 minikube kubelet[1483]: I0614 17:24:13.417650    1483 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="79a9cac2-29d3-4916-a778-8b68a90ce75a" path="/var/lib/kubelet/pods/79a9cac2-29d3-4916-a778-8b68a90ce75a/volumes"
Jun 14 17:26:57 minikube kubelet[1483]: I0614 17:26:57.748424    1483 topology_manager.go:215] "Topology Admit Handler" podUID="4b6aca4e-97ad-4349-9009-2775712f1e66" podNamespace="default" podName="php-deployment-65d55bcf88-9mnw7"
Jun 14 17:26:57 minikube kubelet[1483]: E0614 17:26:57.748567    1483 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="79a9cac2-29d3-4916-a778-8b68a90ce75a" containerName="my-webapp"
Jun 14 17:26:57 minikube kubelet[1483]: I0614 17:26:57.748742    1483 memory_manager.go:354] "RemoveStaleState removing state" podUID="79a9cac2-29d3-4916-a778-8b68a90ce75a" containerName="my-webapp"
Jun 14 17:26:57 minikube kubelet[1483]: I0614 17:26:57.882605    1483 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ktkgv\" (UniqueName: \"kubernetes.io/projected/4b6aca4e-97ad-4349-9009-2775712f1e66-kube-api-access-ktkgv\") pod \"php-deployment-65d55bcf88-9mnw7\" (UID: \"4b6aca4e-97ad-4349-9009-2775712f1e66\") " pod="default/php-deployment-65d55bcf88-9mnw7"
Jun 14 17:27:00 minikube kubelet[1483]: I0614 17:27:00.778652    1483 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/php-deployment-65d55bcf88-9mnw7" podStartSLOduration=2.151729317 podStartE2EDuration="3.778623353s" podCreationTimestamp="2024-06-14 17:26:57 +0000 UTC" firstStartedPulling="2024-06-14 17:26:58.333845662 +0000 UTC m=+9123.542585595" lastFinishedPulling="2024-06-14 17:26:59.960739798 +0000 UTC m=+9125.169479631" observedRunningTime="2024-06-14 17:27:00.778508153 +0000 UTC m=+9125.987248086" watchObservedRunningTime="2024-06-14 17:27:00.778623353 +0000 UTC m=+9125.987363286"
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.318694    1483 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lhs2r\" (UniqueName: \"kubernetes.io/projected/f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b-kube-api-access-lhs2r\") pod \"f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b\" (UID: \"f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b\") "
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.322373    1483 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b-kube-api-access-lhs2r" (OuterVolumeSpecName: "kube-api-access-lhs2r") pod "f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b" (UID: "f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b"). InnerVolumeSpecName "kube-api-access-lhs2r". PluginName "kubernetes.io/projected", VolumeGidValue ""
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.419797    1483 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-lhs2r\" (UniqueName: \"kubernetes.io/projected/f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b-kube-api-access-lhs2r\") on node \"minikube\" DevicePath \"\""
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.815127    1483 scope.go:117] "RemoveContainer" containerID="c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145"
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.843244    1483 scope.go:117] "RemoveContainer" containerID="c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145"
Jun 14 17:27:02 minikube kubelet[1483]: E0614 17:27:02.844568    1483 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145" containerID="c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145"
Jun 14 17:27:02 minikube kubelet[1483]: I0614 17:27:02.844624    1483 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145"} err="failed to get container status \"c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145\": rpc error: code = Unknown desc = Error response from daemon: No such container: c9e4ec5841be34f2317c20d375f009a2b56067780d1b3b217de386dae86f2145"
Jun 14 17:27:03 minikube kubelet[1483]: I0614 17:27:03.419584    1483 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b" path="/var/lib/kubelet/pods/f90f68f7-6aea-49cc-8ad8-a9c1133e6e8b/volumes"


==> storage-provisioner [11f36cc3fe53] <==
I0614 14:55:06.098688       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0614 14:55:16.134159       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout


==> storage-provisioner [7ffdbae0bd40] <==
I0614 14:55:30.693019       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0614 14:55:30.708256       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0614 14:55:30.710264       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0614 14:55:48.118692       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0614 14:55:48.118885       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_247c8cf8-9a90-41d0-a2e6-423717e62fc9!
I0614 14:55:48.118866       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"680ffa96-1d1a-4598-a0fe-7699e73b4205", APIVersion:"v1", ResourceVersion:"6803", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_247c8cf8-9a90-41d0-a2e6-423717e62fc9 became leader
I0614 14:55:48.221058       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_247c8cf8-9a90-41d0-a2e6-423717e62fc9!

